

<!doctype html>
<html lang="en">
  <head>
    
    
    <link href="https://files.realpython.com" rel="preconnect">

    <title>Stochastic Gradient Descent Algorithm With Python and NumPy – Real Python</title>
    <meta name="author" content="Real Python">
    <meta name="description" content="In this tutorial, you&#x27;ll learn what the stochastic gradient descent algorithm is, how it works, and how to implement it with Python and NumPy.">
    <meta name="keywords" content="">

    
  


    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no, viewport-fit=cover">

    
      <link rel="stylesheet" href="/static/realpython.min.aef36e33c64d.css">
      <link rel="stylesheet" href="/static/gfonts/font.5ac42994de49.css">
      <link rel="preload" href="/static/glightbox.min.9b438b29cef1.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="/static/glightbox.min.9b438b29cef1.css"></noscript>
    

    
  
  
  <link rel="preload" as="image" href="https://files.realpython.com/media/Learning-How-Neural-Networks-Learn-Stochastic-Gradient-Descent-Algorithm-With-Python-and-NumPy_Watermarked.140a53be18a9.jpg" imagesrcset="/cdn-cgi/image/width=480,format=auto/https://files.realpython.com/media/Learning-How-Neural-Networks-Learn-Stochastic-Gradient-Descent-Algorithm-With-Python-and-NumPy_Watermarked.140a53be18a9.jpg 480w, /cdn-cgi/image/width=640,format=auto/https://files.realpython.com/media/Learning-How-Neural-Networks-Learn-Stochastic-Gradient-Descent-Algorithm-With-Python-and-NumPy_Watermarked.140a53be18a9.jpg 640w, /cdn-cgi/image/width=960,format=auto/https://files.realpython.com/media/Learning-How-Neural-Networks-Learn-Stochastic-Gradient-Descent-Algorithm-With-Python-and-NumPy_Watermarked.140a53be18a9.jpg 960w, /cdn-cgi/image/width=1920,format=auto/https://files.realpython.com/media/Learning-How-Neural-Networks-Learn-Stochastic-Gradient-Descent-Algorithm-With-Python-and-NumPy_Watermarked.140a53be18a9.jpg 1920w" imagesizes="(min-width: 1200px) 690px, (min-width: 780px) calc(-5vw + 669px), (min-width: 580px) 510px, calc(100vw - 30px)">


    
    
      <link rel="canonical" href="https://realpython.com/gradient-descent-algorithm-python/">

      
      
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:image" content="https://files.realpython.com/media/Learning-How-Neural-Networks-Learn-Stochastic-Gradient-Descent-Algorithm-With-Python-and-NumPy_Watermarked.140a53be18a9.jpg">
        <meta property="og:image" content="https://files.realpython.com/media/Learning-How-Neural-Networks-Learn-Stochastic-Gradient-Descent-Algorithm-With-Python-and-NumPy_Watermarked.140a53be18a9.jpg">
      
      <meta name="twitter:creator" content="@realpython">
      <meta name="twitter:site" content="@realpython">
      <meta property="og:title" content="Stochastic Gradient Descent Algorithm With Python and NumPy – Real Python">
      <meta property="og:type" content="article">
      <meta property="og:url" content="https://realpython.com/gradient-descent-algorithm-python/">
      <meta property="og:description" content="In this tutorial, you&#x27;ll learn what the stochastic gradient descent algorithm is, how it works, and how to implement it with Python and NumPy.">
    

    <link href="/static/favicon.68cbf4197b0c.png" rel="icon">
    <link href="https://realpython.com/atom.xml" rel="alternate" title="Real Python" type="application/atom+xml">
    <link rel="manifest" href="/manifest.json">

    

    
    

    
    <script id="icons-data" type="application/json">{"iconsUrl": "/static/icons.0c2040efe47b.svg"}</script>

    
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  

  ga('create', 'UA-35184939-1', 'auto', {'allowLinker': true});

  

  

  
  
  ga('set', {
    dimension1: false,
    dimension2: false
  });
  

  ga('send', 'pageview');
  
</script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-L7L6L5F6Y6"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-L7L6L5F6Y6');

  
</script>

  </head>
  <body >
    
      



<nav class="navbar fixed-top navbar-expand-lg navbar-dark flex-column ">
  <div class="container flex-row">

    <a class="navbar-brand" href="/">
      <img src="/static/real-python-logo.893c30edea53.svg" width="165" height="40" class="d-inline-block align-top" alt="Real Python">
    </a>

    
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse navbar-nav-scroll" id="navbarSupportedContent" role="navigation" aria-label="Main Navigation">

      <ul class="navbar-nav mr-2">

        <li class="nav-item">
          <a class="nav-link" href="/start-here/">Start&nbsp;Here</a>
        </li>

        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#" id="navbarDropdownLibrary" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
            <span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#brands--python"/></svg></span> Learn Python
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdownLibrary">
            
<a class="dropdown-item " href="/search?kind=article&kind=course&order=newest" style="color: #ff7e73; line-height: 110%;"><span class="icon baseline mr-1"><svg><use href="/static/icons.0c2040efe47b.svg#solid--graduation-cap"/></svg></span>Python Tutorials&nbsp;→<br><small class="text-secondary">In-depth articles and video courses</small></a>


            
<a class="dropdown-item " href="/learning-paths/" style="color: #ffc873; line-height: 110%;"><span class="icon baseline mr-1"><svg><use href="/static/icons.0c2040efe47b.svg#@learning-path"/></svg></span>Learning Paths&nbsp;→<br><small class="text-secondary">Guided study plans for accelerated learning</small></a>


            
<a class="dropdown-item " href="/quizzes/" style="color: #abe0e5; line-height: 110%;"><span class="icon baseline mr-1"><svg><use href="/static/icons.0c2040efe47b.svg#@quiz"/></svg></span>Quizzes&nbsp;→<br><small class="text-secondary">Check your learning progress</small></a>


            
<a class="dropdown-item " href="/tutorials/all/" style="color: #ccc; line-height: 110%;"><span class="icon baseline mr-1"><svg><use href="/static/icons.0c2040efe47b.svg#@category"/></svg></span>Browse Topics&nbsp;→<br><small class="text-secondary">Focus on a specific area or skill level</small></a>


            
<a class="dropdown-item " href="/community/" style="color: #e5c6ab; line-height: 110%;"><span class="icon baseline mr-1"><svg><use href="/static/icons.0c2040efe47b.svg#solid--message-smile"/></svg></span>Community Chat&nbsp;→<br><small class="text-secondary">Learn with other Pythonistas</small></a>


            
<a class="dropdown-item " href="/office-hours/" style="color: #e5c6ab; line-height: 110%;"><span class="icon baseline mr-1"><svg><use href="/static/icons.0c2040efe47b.svg#solid--users"/></svg></span>Office Hours&nbsp;→<br><small class="text-secondary">Live Q&A calls with Python experts</small></a>


            
<a class="dropdown-item " href="/podcasts/rpp/" style="color: #b8abe5; line-height: 110%;"><span class="icon baseline mr-1"><svg><use href="/static/icons.0c2040efe47b.svg#@podcast"/></svg></span>Podcast&nbsp;→<br><small class="text-secondary">Hear what’s new in the world of Python</small></a>


            
<a class="dropdown-item " href="/products/books/" style="color: #abe5b1; line-height: 110%;"><span class="icon baseline mr-1"><svg><use href="/static/icons.0c2040efe47b.svg#solid--book-heart"/></svg></span>Books&nbsp;→<br><small class="text-secondary">Round out your knowledge and learn offline</small></a>


            
              
              
<a class="dropdown-item border-top text-warning" href="/account/join/" style="line-height: 110%;"><span class="icon baseline mr-1"><svg><use href="/static/icons.0c2040efe47b.svg#solid--star"/></svg></span>Unlock All Content&nbsp;→</a>

            
          </div>
        </li>

        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#" id="navbarDropdownMore" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
            More
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdownMore">
            <a class="dropdown-item" href="/learner-stories/">Learner Stories</a>
            <a class="dropdown-item" href="/newsletter/">Python Newsletter</a>
            <a class="dropdown-item" href="https://www.pythonjobshq.com" target="_blank">Python Job Board</a>
            <a class="dropdown-item" href="/team/">Meet the Team</a>
            <a class="dropdown-item" href="/write-for-us/">Become a Tutorial Writer</a>
            <a class="dropdown-item" href="/become-an-instructor/">Become a Video Instructor</a>
          </div>
        </li>

      </ul> 

      
      <div class="d-block d-lg-none"> 
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link" href="/search" title="Search"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#v4--search"/></svg></span> Search</a>
          </li>
        </ul>
      </div>
      
      <div class="d-none d-lg-flex align-items-center mr-2 flex-fill">
        <form class="form-inline w-100 position-relative" action="/search" method="GET">
          <a class="js-search-form-submit position-absolute" href="/search" title="Search"><span class="icon baseline text-muted pl-2"><svg><use href="/static/icons.0c2040efe47b.svg#v4--search"/></svg></span></a>
          <input class="search-autocomplete search-field form-control form-control-md mr-sm-1 mr-lg-2 w-100" style="padding-left: 2rem;" maxlength=50 type="search" placeholder="Search" aria-label="Search" name="q" autocomplete="off">
          <span class="search-hotkey-indicator position-absolute px-2 border border-input text-input rounded small user-select-none" style="right: 1em;" title="Press / (forward slash) or Ctrl+J to open search">/</span>
          <input type="hidden" name="_from" value="nav">
        </form>
      </div>
      

      <ul class="navbar-nav ml-auto">
        
          <li class="nav-item form-inline">
            <a class="ml-2 ml-lg-0 btn btn-sm btn-primary px-3" href="/account/join/">Join</a>
          </li>
          <li class="nav-item">
            <a class="btn text-light" href="/account/login/?next=%2Fgradient-descent-algorithm-python%2F">Sign&#8209;In</a>
          </li>
        
      </ul>

    
    </div>

  </div>

  

</nav>

    

    <div class="container main-content">
      

      
  <div class="row justify-content-center">

    
    <aside class="col-md-7 col-lg-4 order-2 d-none d-lg-block">
      
  




<div class="card mb-3 bg-secondary">
  <form class="card-body" action="/optins/process/" method="post">
    <div class="form-group">
      <p class="h5 text-muted text-center">— FREE Email Series —</p>
      <p class="h3 text-center">🐍 Python Tricks 💌</p>
      <p><img loading="lazy" class="img-fluid rounded" src="/static/pytrick-dict-merge.4201a0125a5e.png" width="738" height="490" alt="Python Tricks Dictionary Merge"></p>
    </div>
    <div class="form-group">
      <input type="hidden" name="csrfmiddlewaretoken" value="WMo7jZKy0VzLokBMAryCrBUj4p6InOrGDcUwE7VRo9NWcEXEivCM7yz9u9xak2EP">
      <input type="hidden" name="slug" value="static-python-tricks-sidebar">
      <input type="email" class="form-control form-control-md" name="email" placeholder="Email&hellip;" required>
    </div>
    <button type="submit" name="submit" class="btn btn-primary btn-md btn-block">Get Python Tricks »</button>
    <p class="mb-0 mt-2 text-muted text-center">🔒 No spam. Unsubscribe any time.</p>
  </form>
</div>


  


<div class="sidebar-module sidebar-module-inset border">
  <a href="/tutorials/all/" class="badge badge-light text-muted"><span class="icon baseline mr-1"><svg><use href="/static/icons.0c2040efe47b.svg#@category"/></svg></span>Browse Topics</a>

  <a href="/learning-paths/" class="badge badge-light text-muted"><span class="icon baseline mr-1"><svg><use href="/static/icons.0c2040efe47b.svg#solid--map-location-dot"/></svg></span>Guided Learning Paths</a>

  <br>

  <a href="/search?level=basics" class="badge badge-light text-muted"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#v4--angle-double-up"/></svg></span> Basics</a>

  <a href="/search?level=intermediate" class="badge badge-light text-muted"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#v4--angle-double-up"/></svg></span><span class="icon baseline tight-left"><svg><use href="/static/icons.0c2040efe47b.svg#v4--angle-double-up"/></svg></span> Intermediate</a>

  <a href="/search?level=advanced" class="badge badge-light text-muted"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#v4--angle-double-up"/></svg></span><span class="icon baseline tight-left"><svg><use href="/static/icons.0c2040efe47b.svg#v4--angle-double-up"/></svg></span><span class="icon baseline tight-left"><svg><use href="/static/icons.0c2040efe47b.svg#v4--angle-double-up"/></svg></span> Advanced</a>

  <hr class="my-2">

  
    
  
    
    <a href="/tutorials/api/" class="badge badge-light text-muted">api</a>
    
  
    
  
    
    <a href="/tutorials/best-practices/" class="badge badge-light text-muted">best-practices</a>
    
  
    
    <a href="/tutorials/career/" class="badge badge-light text-muted">career</a>
    
  
    
    <a href="/tutorials/community/" class="badge badge-light text-muted">community</a>
    
  
    
    <a href="/tutorials/databases/" class="badge badge-light text-muted">databases</a>
    
  
    
    <a href="/tutorials/data-science/" class="badge badge-light text-muted">data-science</a>
    
  
    
    <a href="/tutorials/data-structures/" class="badge badge-light text-muted">data-structures</a>
    
  
    
    <a href="/tutorials/data-viz/" class="badge badge-light text-muted">data-viz</a>
    
  
    
    <a href="/tutorials/devops/" class="badge badge-light text-muted">devops</a>
    
  
    
    <a href="/tutorials/django/" class="badge badge-light text-muted">django</a>
    
  
    
    <a href="/tutorials/docker/" class="badge badge-light text-muted">docker</a>
    
  
    
    <a href="/tutorials/editors/" class="badge badge-light text-muted">editors</a>
    
  
    
    <a href="/tutorials/flask/" class="badge badge-light text-muted">flask</a>
    
  
    
    <a href="/tutorials/front-end/" class="badge badge-light text-muted">front-end</a>
    
  
    
    <a href="/tutorials/gamedev/" class="badge badge-light text-muted">gamedev</a>
    
  
    
    <a href="/tutorials/gui/" class="badge badge-light text-muted">gui</a>
    
  
    
  
    
    <a href="/tutorials/machine-learning/" class="badge badge-light text-muted">machine-learning</a>
    
  
    
    <a href="/tutorials/numpy/" class="badge badge-light text-muted">numpy</a>
    
  
    
    <a href="/tutorials/projects/" class="badge badge-light text-muted">projects</a>
    
  
    
    <a href="/tutorials/python/" class="badge badge-light text-muted">python</a>
    
  
    
    <a href="/tutorials/testing/" class="badge badge-light text-muted">testing</a>
    
  
    
    <a href="/tutorials/tools/" class="badge badge-light text-muted">tools</a>
    
  
    
    <a href="/tutorials/web-dev/" class="badge badge-light text-muted">web-dev</a>
    
  
    
    <a href="/tutorials/web-scraping/" class="badge badge-light text-muted">web-scraping</a>
    
  
</div>



  
<div class="sidebar-module sidebar-module-inset p-0" style="overflow:hidden;">
  
<div style="display:block;position:relative;">
  <div style="display:block;width:100%;padding-top:100%;"></div>
  <div class="rpad" data-unit="1x1" style="position:absolute;left:0;top:0;right:0;bottom:0;overflow:hidden;"></div>
</div>


</div>



  <div class="sidebar-sticky ">
    
      <div class="bg-light sidebar-module sidebar-module-inset" id="sidebar-toc">
        <p class="h4 text-muted"><a class="link-unstyled" href="#toc">Table of Contents</a></p>
        <div class="toc">
<ul>
<li><a href="#basic-gradient-descent-algorithm">Basic Gradient Descent Algorithm</a><ul>
<li><a href="#cost-function-the-goal-of-optimization">Cost Function: The Goal of Optimization</a></li>
<li><a href="#gradient-of-a-function-calculus-refresher">Gradient of a Function: Calculus Refresher</a></li>
<li><a href="#intuition-behind-gradient-descent">Intuition Behind Gradient Descent</a></li>
<li><a href="#implementation-of-basic-gradient-descent">Implementation of Basic Gradient Descent</a></li>
<li><a href="#learning-rate-impact">Learning Rate Impact</a></li>
</ul>
</li>
<li><a href="#application-of-the-gradient-descent-algorithm">Application of the Gradient Descent Algorithm</a><ul>
<li><a href="#short-examples">Short Examples</a></li>
<li><a href="#ordinary-least-squares">Ordinary Least Squares</a></li>
<li><a href="#improvement-of-the-code">Improvement of the Code</a></li>
</ul>
</li>
<li><a href="#stochastic-gradient-descent-algorithms">Stochastic Gradient Descent Algorithms</a><ul>
<li><a href="#minibatches-in-stochastic-gradient-descent">Minibatches in Stochastic Gradient Descent</a></li>
<li><a href="#momentum-in-stochastic-gradient-descent">Momentum in Stochastic Gradient Descent</a></li>
<li><a href="#random-start-values">Random Start Values</a></li>
</ul>
</li>
<li><a href="#gradient-descent-in-keras-and-tensorflow">Gradient Descent in Keras and TensorFlow</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</div>

      </div>
    

    <div class="sidebar-module sidebar-module-inset text-center my-3 py-0">
      
<div class="jsCompletionStatusWidget btn-group mb-0">
  <button title="Click to mark as completed" class="jsBtnCompletion btn btn-secondary border-right " style="border-top-right-radius: 0; border-bottom-right-radius: 0;" disabled>Mark as Completed</button>
  <button title="Add bookmark" class="jsBtnBookmark btn btn-secondary border-left" disabled><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#light--bookmark"/></svg></span></button>
</div>

      <div class="my-2">
        
<div class="btn-group mb-0">
  <a class="btn btn-secondary border-right" style="border-top-right-radius: 0; border-bottom-right-radius: 0;" title="Liked it" role="button" aria-label="Thumbs up (liked it)" href="/feedback/survey/article/gradient-descent-algorithm-python/liked/?from=article-sidebar" target="_blank"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#light--thumbs-up"/></svg></span></a>
  <a class="btn btn-secondary border-left" role="button" aria-label="Thumbs down (disliked it)" title="Disliked it" href="/feedback/survey/article/gradient-descent-algorithm-python/disliked/?from=article-sidebar" target="_blank"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#light--thumbs-down"/></svg></span></a>
</div>

      </div>
    </div>

    
      <div class="sidebar-module sidebar-module-inset text-center my-3 py-0">
        

<span>
  <a target="_blank" rel="nofollow" href="https://twitter.com/intent/tweet/?text=Check out this %23Python tutorial: Stochastic%20Gradient%20Descent%20Algorithm%20With%20Python%20and%20NumPy by @realpython&url=https%3A//realpython.com/gradient-descent-algorithm-python/" class="mr-1 badge badge-x-twitter text-light mb-1"><span class="icon baseline mr-1 text-light"><svg><use href="/static/icons.0c2040efe47b.svg#brands--x-twitter"/></svg></span>Share</a>
  <a target="_blank" rel="nofollow" href="https://facebook.com/sharer/sharer.php?u=https%3A//realpython.com/gradient-descent-algorithm-python/" class="mr-1 badge badge-facebook text-light mb-1"><span class="icon baseline mr-1 text-light"><svg><use href="/static/icons.0c2040efe47b.svg#brands--facebook"/></svg></span>Share</a>
  
  <a target="_blank" rel="nofollow" href="mailto:?subject=Python article for you&body=Check out this Python tutorial:%0A%0AStochastic%20Gradient%20Descent%20Algorithm%20With%20Python%20and%20NumPy%0A%0Ahttps%3A//realpython.com/gradient-descent-algorithm-python/" class="badge badge-red text-light mb-1"><span class="icon baseline mr-1 text-light"><svg><use href="/static/icons.0c2040efe47b.svg#solid--envelope"/></svg></span>Email</a>
</span>

      </div>
    

    

    
      <div class="sidebar-module sidebar-module-inset p-0" style="overflow:hidden;">
        
<div style="display:block;position:relative;">
  <div style="display:block;width:100%;padding-top:25%;"></div>
  <div class="rpad" data-unit="4x1" style="position:absolute;left:0;top:0;right:0;bottom:0;overflow:hidden;"></div>
</div>


      </div>
    

    
  </div>


    </aside>

    <div class="col-md-11 col-lg-8 article with-headerlinks">
      

  
    <figure class="embed-responsive embed-responsive-16by9 rounded">
      
        <img class="card-img-top m-0 p-0 embed-responsive-item" style="object-fit: contain; background: #ffc873;" alt="Stochastic Gradient Descent Algorithm With Python and NumPy" width="1920" height="1080" src="https://files.realpython.com/media/Learning-How-Neural-Networks-Learn-Stochastic-Gradient-Descent-Algorithm-With-Python-and-NumPy_Watermarked.140a53be18a9.jpg" srcset="/cdn-cgi/image/width=480,format=auto/https://files.realpython.com/media/Learning-How-Neural-Networks-Learn-Stochastic-Gradient-Descent-Algorithm-With-Python-and-NumPy_Watermarked.140a53be18a9.jpg 480w, /cdn-cgi/image/width=640,format=auto/https://files.realpython.com/media/Learning-How-Neural-Networks-Learn-Stochastic-Gradient-Descent-Algorithm-With-Python-and-NumPy_Watermarked.140a53be18a9.jpg 640w, /cdn-cgi/image/width=960,format=auto/https://files.realpython.com/media/Learning-How-Neural-Networks-Learn-Stochastic-Gradient-Descent-Algorithm-With-Python-and-NumPy_Watermarked.140a53be18a9.jpg 960w, /cdn-cgi/image/width=1920,format=auto/https://files.realpython.com/media/Learning-How-Neural-Networks-Learn-Stochastic-Gradient-Descent-Algorithm-With-Python-and-NumPy_Watermarked.140a53be18a9.jpg 1920w" sizes="(min-width: 1200px) 690px, (min-width: 780px) calc(-5vw + 669px), (min-width: 580px) 510px, calc(100vw - 30px)" fetchpriority="high">
      
    </figure>
  

  <h1>Stochastic Gradient Descent Algorithm With Python and NumPy</h1>
  <div class="mb-0">
    
      <span class="text-muted">by <a class="text-muted" href="#author">Mirko Stojiljković</a>
    
    
    
      <span class="icon baseline ml-2 mr-1"><svg><use href="/static/icons.0c2040efe47b.svg#solid--comments"/></svg></span><a class="text-muted" href="#reader-comments"><span class="disqus-comment-count" data-disqus-identifier="https://realpython.com/gradient-descent-algorithm-python/"></span></a>
    
    

    <span class="d-inline d-md-block"> 
      <span class="icon baseline ml-2 ml-md-0"><svg><use href="/static/icons.0c2040efe47b.svg#@category"/></svg></span>
      
        <a href="/tutorials/advanced/" class="badge badge-light text-muted">advanced</a>
      
        <a href="/tutorials/machine-learning/" class="badge badge-light text-muted">machine-learning</a>
      
        <a href="/tutorials/numpy/" class="badge badge-light text-muted">numpy</a>
      
    </span>

    <div class="d-sm-flex flex-row justify-content-between my-3 text-center">
      
<div class="jsCompletionStatusWidget btn-group mb-0">
  <button title="Click to mark as completed" class="jsBtnCompletion btn btn-secondary border-right " style="border-top-right-radius: 0; border-bottom-right-radius: 0;" disabled>Mark as Completed</button>
  <button title="Add bookmark" class="jsBtnBookmark btn btn-secondary border-left" disabled><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#light--bookmark"/></svg></span></button>
</div>

      
      
        <div class="align-self-center my-2">
          

<span>
  <a target="_blank" rel="nofollow" href="https://twitter.com/intent/tweet/?text=Check out this %23Python tutorial: Stochastic%20Gradient%20Descent%20Algorithm%20With%20Python%20and%20NumPy by @realpython&url=https%3A//realpython.com/gradient-descent-algorithm-python/" class="mr-1 badge badge-x-twitter text-light mb-1"><span class="icon baseline mr-1 text-light"><svg><use href="/static/icons.0c2040efe47b.svg#brands--x-twitter"/></svg></span>Share</a>
  <a target="_blank" rel="nofollow" href="https://facebook.com/sharer/sharer.php?u=https%3A//realpython.com/gradient-descent-algorithm-python/" class="mr-1 badge badge-facebook text-light mb-1"><span class="icon baseline mr-1 text-light"><svg><use href="/static/icons.0c2040efe47b.svg#brands--facebook"/></svg></span>Share</a>
  
  <a target="_blank" rel="nofollow" href="mailto:?subject=Python article for you&body=Check out this Python tutorial:%0A%0AStochastic%20Gradient%20Descent%20Algorithm%20With%20Python%20and%20NumPy%0A%0Ahttps%3A//realpython.com/gradient-descent-algorithm-python/" class="badge badge-red text-light mb-1"><span class="icon baseline mr-1 text-light"><svg><use href="/static/icons.0c2040efe47b.svg#solid--envelope"/></svg></span>Email</a>
</span>

        </div>
      
    </div>
  </div>

  

  <div class="article-body">
    
      <div class="bg-light sidebar-module sidebar-module-inset" id="toc">
        <p class="h3 mb-2 text-muted">Table of Contents</p>
        <div class="toc">
<ul>
<li><a href="#basic-gradient-descent-algorithm">Basic Gradient Descent Algorithm</a><ul>
<li><a href="#cost-function-the-goal-of-optimization">Cost Function: The Goal of Optimization</a></li>
<li><a href="#gradient-of-a-function-calculus-refresher">Gradient of a Function: Calculus Refresher</a></li>
<li><a href="#intuition-behind-gradient-descent">Intuition Behind Gradient Descent</a></li>
<li><a href="#implementation-of-basic-gradient-descent">Implementation of Basic Gradient Descent</a></li>
<li><a href="#learning-rate-impact">Learning Rate Impact</a></li>
</ul>
</li>
<li><a href="#application-of-the-gradient-descent-algorithm">Application of the Gradient Descent Algorithm</a><ul>
<li><a href="#short-examples">Short Examples</a></li>
<li><a href="#ordinary-least-squares">Ordinary Least Squares</a></li>
<li><a href="#improvement-of-the-code">Improvement of the Code</a></li>
</ul>
</li>
<li><a href="#stochastic-gradient-descent-algorithms">Stochastic Gradient Descent Algorithms</a><ul>
<li><a href="#minibatches-in-stochastic-gradient-descent">Minibatches in Stochastic Gradient Descent</a></li>
<li><a href="#momentum-in-stochastic-gradient-descent">Momentum in Stochastic Gradient Descent</a></li>
<li><a href="#random-start-values">Random Start Values</a></li>
</ul>
</li>
<li><a href="#gradient-descent-in-keras-and-tensorflow">Gradient Descent in Keras and TensorFlow</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</div>

      </div>
    

    
    <div class="sidebar-module sidebar-module-inset p-0" style="overflow:hidden;">
      

<div style="display:block;position:relative;">
  <div style="display:block;width:100%;padding-top:12.5%;"></div>
  <div class="rpad rounded border" data-unit="8x1" style="position:absolute;left:0;top:0;right:0;bottom:0;overflow:hidden;"></div>
</div>
<a class="small text-muted" href="/account/join/" rel="nofollow"><span class="icon baseline mr-1"><svg><use href="/static/icons.0c2040efe47b.svg#solid--circle-info"/></svg></span>Remove ads</a>


    </div>
    

    

    <p><a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent"><strong>Stochastic gradient descent</strong></a> is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs. It&rsquo;s an inexact but powerful technique.</p>
<p>Stochastic gradient descent is widely used in machine learning applications. Combined with <a href="https://brilliant.org/wiki/backpropagation/">backpropagation</a>, it&rsquo;s dominant in <a href="https://realpython.com/python-keras-text-classification/#a-primer-on-deep-neural-networks">neural network</a> training applications.</p>
<p><strong>In this tutorial, you&rsquo;ll learn:</strong></p>
<ul>
<li>How <strong>gradient descent</strong> and <strong>stochastic gradient descent</strong> algorithms work</li>
<li>How to apply gradient descent and stochastic gradient descent to <strong>minimize the loss function</strong> in machine learning</li>
<li>What the <strong>learning rate</strong> is, why it&rsquo;s important, and how it impacts results</li>
<li>How to <strong>write your own function</strong> for stochastic gradient descent</li>
</ul>
<div class="alert alert-warning" role="alert">
<p><strong markdown="1">Free Bonus:</strong> <a href="https://realpython.com/bonus/python-mastery-course/" class="alert-link" data-toggle="modal" data-target="#modal-python-mastery-course" data-focus="false" markdown="1">5 Thoughts On Python Mastery</a>, a free course for Python developers that shows you the roadmap and the mindset you&rsquo;ll need to take your Python skills to the next level.</p>
</div>
<section class="section2"><h2 id="basic-gradient-descent-algorithm">Basic Gradient Descent Algorithm<a class="headerlink" href="#basic-gradient-descent-algorithm" title="Permanent link"></a></h2>
<p>The <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent algorithm</a> is an approximate and iterative method for <a href="https://en.wikipedia.org/wiki/Mathematical_optimization">mathematical optimization</a>. You can use it to approach the minimum of any <a href="https://en.wikipedia.org/wiki/Differentiable_function">differentiable function</a>.</p>
<div class="alert alert-primary" role="alert">
<p><strong>Note:</strong> There are many optimization methods and <a href="https://en.wikipedia.org/wiki/Mathematical_optimization#Major_subfields">subfields of mathematical programming</a>. If you want to learn how to use some of them with Python, then check out <a href="https://realpython.com/python-scipy-cluster-optimize/">Scientific Python: Using SciPy for Optimization</a> and <a href="https://realpython.com/linear-programming-python/">Hands-On Linear Programming: Optimization With Python</a>.</p>
</div>
<p>Although gradient descent sometimes gets stuck in a <a href="https://en.wikipedia.org/wiki/Local_optimum">local minimum</a> or a <a href="https://en.wikipedia.org/wiki/Saddle_point">saddle point</a> instead of finding the global minimum, it&rsquo;s widely used in practice. <a href="https://realpython.com/learning-paths/data-science-python-core-skills/">Data science</a> and <a href="https://realpython.com/learning-paths/machine-learning-python/">machine learning</a> methods often apply it internally to optimize model parameters. For example, neural networks find <a href="https://docs.paperspace.com/machine-learning/wiki/weights-and-biases">weights and biases</a> with gradient descent.</p>
<div><div class="rounded border border-light" style="display:block;position:relative;"> <div style="display:block;width:100%;padding-top:12.5%;"></div> <div class="rpad rounded border" data-unit="8x1" style="position:absolute;left:0;top:0;right:0;bottom:0;overflow:hidden;"> </div></div><a class="small text-muted" href="/account/join/" rel="nofollow"> <span class="icon baseline mr-1"><svg><use href="/static/icons.0c2040efe47b.svg#solid--circle-info"></use></svg></span>Remove ads</a></div><section class="section3"><h3 id="cost-function-the-goal-of-optimization">Cost Function: The Goal of Optimization<a class="headerlink" href="#cost-function-the-goal-of-optimization" title="Permanent link"></a></h3>
<p>The <strong>cost function</strong>, or <a href="https://en.wikipedia.org/wiki/Loss_function">loss function</a>, is the function to be minimized (or maximized) by varying the decision variables. Many machine learning methods solve optimization problems under the surface. They tend to minimize the difference between actual and predicted outputs by adjusting the model parameters (like weights and biases for <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural networks</a>, decision rules for <a href="https://en.wikipedia.org/wiki/Random_forest">random forest</a> or <a href="https://en.wikipedia.org/wiki/Gradient_boosting">gradient boosting</a>, and so on).</p>
<p>In a <a href="https://realpython.com/linear-regression-in-python/#regression">regression problem</a>, you typically have the vectors of input variables 𝐱 = (𝑥₁, …, 𝑥ᵣ) and the actual outputs 𝑦. You want to find a model that maps 𝐱 to a predicted response 𝑓(𝐱) so that 𝑓(𝐱) is as close as possible to 𝑦. For example, you might want to predict an output such as a person&rsquo;s salary given inputs like the person&rsquo;s number of years at the company or level of education.</p>
<p>Your goal is to minimize the difference between the prediction 𝑓(𝐱) and the actual data 𝑦. This difference is called the <strong>residual</strong>.</p>
<p>In this type of problem, you want to minimize the <a href="https://en.wikipedia.org/wiki/Residual_sum_of_squares">sum of squared residuals (SSR)</a>, where SSR = Σᵢ(𝑦ᵢ − 𝑓(𝐱ᵢ))² for all observations 𝑖 = 1, …, 𝑛, where 𝑛 is the total number of observations. Alternatively, you could use the <a href="https://en.wikipedia.org/wiki/Mean_squared_error">mean squared error</a> (MSE = SSR / 𝑛) instead of SSR.</p>
<p>Both SSR and MSE use the square of the difference between the actual and predicted outputs. The lower the difference, the more accurate the prediction. A difference of zero indicates that the prediction is equal to the actual data.</p>
<p>SSR or MSE is minimized by adjusting the model parameters. For example, in <a href="https://realpython.com/linear-regression-in-python/">linear regression</a>, you want to find the function 𝑓(𝐱) = 𝑏₀ + 𝑏₁𝑥₁ + ⋯ + 𝑏ᵣ𝑥ᵣ, so you need to determine the weights 𝑏₀, 𝑏₁, …, 𝑏ᵣ that minimize SSR or MSE.</p>
<p>In a <a href="https://realpython.com/logistic-regression-python/#classification">classification problem</a>, the outputs 𝑦 are <a href="https://en.wikipedia.org/wiki/Categorical_variable">categorical</a>, often either 0 or 1. For example, you might try to predict whether an email is spam or not. In the case of binary outputs, it&rsquo;s convenient to minimize the <a href="https://en.wikipedia.org/wiki/Cross_entropy">cross-entropy function</a> that also depends on the actual outputs 𝑦ᵢ and the corresponding predictions 𝑝(𝐱ᵢ):</p>
<figure class="js-lightbox"><a href="https://files.realpython.com/media/mmst-gda-eqs-1.119ab87cc186.png" target="_blank"><img loading="lazy" class="img-fluid mx-auto d-block w-66" src="https://files.realpython.com/media/mmst-gda-eqs-1.119ab87cc186.png" width="1816" height="194" srcset="https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/mmst-gda-eqs-1.119ab87cc186.png&amp;w=454&amp;sig=e6d8b3e4d147738a8c0ccebf2e4747b5b63b74a9 454w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/mmst-gda-eqs-1.119ab87cc186.png&amp;w=605&amp;sig=7344db91ae9aab7f7f71ddd2ee5fb6454f0c137b 605w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/mmst-gda-eqs-1.119ab87cc186.png&amp;w=908&amp;sig=1558fcfcdbaf30b23a55ae380291769b090d1194 908w, https://files.realpython.com/media/mmst-gda-eqs-1.119ab87cc186.png 1816w" sizes="(min-width: 1200px) 690px, (min-width: 780px) calc(-5vw + 669px), (min-width: 580px) 510px, calc(100vw - 30px)" alt="mmst-gda-eqs-1" data-asset="3400"/></a></figure>

<p>In <a href="https://realpython.com/logistic-regression-python/">logistic regression</a>, which is often used to solve classification problems, the functions 𝑝(𝐱) and 𝑓(𝐱) are defined as the following:</p>
<figure class="js-lightbox"><a href="https://files.realpython.com/media/mmst-gda-eqs-2.76aa15da2cc0.png" target="_blank"><img loading="lazy" class="img-fluid mx-auto d-block w-66" src="https://files.realpython.com/media/mmst-gda-eqs-2.76aa15da2cc0.png" width="1820" height="317" srcset="https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/mmst-gda-eqs-2.76aa15da2cc0.png&amp;w=455&amp;sig=8d933a1259c626545ac1760d9fdbb509c1efe61e 455w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/mmst-gda-eqs-2.76aa15da2cc0.png&amp;w=606&amp;sig=e5d8e0499b023a3339faa6c47190384b4ef0b658 606w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/mmst-gda-eqs-2.76aa15da2cc0.png&amp;w=910&amp;sig=c961e1c3c89347d7084083609cb267b54dfb67be 910w, https://files.realpython.com/media/mmst-gda-eqs-2.76aa15da2cc0.png 1820w" sizes="(min-width: 1200px) 690px, (min-width: 780px) calc(-5vw + 669px), (min-width: 580px) 510px, calc(100vw - 30px)" alt="mmst-gda-eqs-2" data-asset="3401"/></a></figure>

<p>Again, you need to find the weights 𝑏₀, 𝑏₁, …, 𝑏ᵣ, but this time they should minimize the cross-entropy function.</p>
</section><section class="section3"><h3 id="gradient-of-a-function-calculus-refresher">Gradient of a Function: Calculus Refresher<a class="headerlink" href="#gradient-of-a-function-calculus-refresher" title="Permanent link"></a></h3>
<p>In calculus, the <a href="https://www.mathsisfun.com/calculus/derivatives-introduction.html">derivative</a> of a function shows you how much a value changes when you modify its argument (or arguments). Derivatives are important for optimization because the <a href="http://sofia.nmsu.edu/~breakingaway/ebookofcalculus/MeaningOfDerivativesAndIntegrals/WhatDoesItMeanThatTheDerivativeOfAFunctionEquals0/WhatDoesItMeanThatTheDerivativeOfAFunctionEquals0.html">zero derivatives</a> might indicate a minimum, maximum, or saddle point.</p>
<p>The <a href="https://en.wikipedia.org/wiki/Gradient">gradient</a> of a function 𝐶 of several independent variables 𝑣₁, …, 𝑣ᵣ is denoted with ∇𝐶(𝑣₁, …, 𝑣ᵣ) and defined as the vector function of the <a href="https://en.wikipedia.org/wiki/Partial_derivative">partial derivatives</a> of 𝐶 with respect to each independent variable: ∇𝐶 = (∂𝐶/∂𝑣₁, …, ∂𝐶/𝑣ᵣ). The symbol ∇ is called <a href="https://en.wikipedia.org/wiki/Nabla_symbol">nabla</a>.</p>
<p>The nonzero value of the gradient of a function 𝐶 at a given point defines the direction and rate of the fastest increase of 𝐶. When working with gradient descent, you&rsquo;re interested in the direction of the fastest <em>decrease</em> in the cost function. This direction is determined by the negative gradient, −∇𝐶.</p>
</section><section class="section3"><h3 id="intuition-behind-gradient-descent">Intuition Behind Gradient Descent<a class="headerlink" href="#intuition-behind-gradient-descent" title="Permanent link"></a></h3>
<p>To understand the gradient descent algorithm, imagine a drop of water sliding down the side of a bowl or a ball rolling down a hill. The drop and the ball tend to move in the direction of the fastest decrease until they reach the bottom. With time, they&rsquo;ll gain momentum and accelerate.</p>
<p>The idea behind gradient descent is similar: you start with an arbitrarily chosen position of the point or vector 𝐯 = (𝑣₁, …, 𝑣ᵣ) and move it iteratively in the direction of the fastest decrease of the cost function. As mentioned, this is the direction of the negative gradient vector, −∇𝐶.</p>
<p>Once you have a random starting point 𝐯 = (𝑣₁, …, 𝑣ᵣ), you <strong>update</strong> it, or move it to a new position in the direction of the negative gradient: 𝐯 → 𝐯 − 𝜂∇𝐶, where 𝜂 (pronounced &ldquo;ee-tah&rdquo;) is a small positive value called the <strong>learning rate</strong>.</p>
<p>The learning rate determines how large the update or moving step is. It&rsquo;s a very important parameter. If 𝜂 is too small, then the algorithm might converge very slowly. Large 𝜂 values can also cause issues with convergence or make the algorithm divergent.</p>
<div><div class="rounded border border-light" style="display:block;position:relative;"> <div style="display:block;width:100%;padding-top:12.5%;"></div> <div class="rpad rounded border" data-unit="8x1" style="position:absolute;left:0;top:0;right:0;bottom:0;overflow:hidden;"> </div></div><a class="small text-muted" href="/account/join/" rel="nofollow"> <span class="icon baseline mr-1"><svg><use href="/static/icons.0c2040efe47b.svg#solid--circle-info"></use></svg></span>Remove ads</a></div></section><section class="section3"><h3 id="implementation-of-basic-gradient-descent">Implementation of Basic Gradient Descent<a class="headerlink" href="#implementation-of-basic-gradient-descent" title="Permanent link"></a></h3>
<p>Now that you know how the basic gradient descent works, you can implement it in Python. You&rsquo;ll use only plain Python and <a href="https://numpy.org/">NumPy</a>, which enables you to write <a href="https://realpython.com/numpy-array-programming/">concise code</a> when working with arrays (or vectors) and gain a <a href="https://realpython.com/numpy-tensorflow-performance/">performance boost</a>.</p>
<p>This is a basic implementation of the algorithm that starts with an arbitrary point, <code>start</code>, iteratively moves it toward the minimum, and <a href="https://realpython.com/python-return-statement/">returns</a> a point that is hopefully at or near the minimum:</p>
<div class="codeblock mb-3 w-100" aria-label="Code block">
  <div class="codeblock__header d-flex justify-content-between codeblock--blue">
    <span class="mr-2 noselect" aria-label="Language">Python</span>
    
    <div class="noselect">
      
    </div>
  </div>
  <div style="position: relative;">
    <div class="highlight python highlight--with-header"><pre><span></span><code><span class="linenos"> 1</span><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">gradient</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">learn_rate</span><span class="p">,</span> <span class="n">n_iter</span><span class="p">):</span>
<span class="linenos"> 2</span>    <span class="n">vector</span> <span class="o">=</span> <span class="n">start</span>
<span class="linenos"> 3</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
<span class="linenos"> 4</span>        <span class="n">diff</span> <span class="o">=</span> <span class="o">-</span><span class="n">learn_rate</span> <span class="o">*</span> <span class="n">gradient</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span>
<span class="linenos"> 5</span>        <span class="n">vector</span> <span class="o">+=</span> <span class="n">diff</span>
<span class="linenos"> 6</span>    <span class="k">return</span> <span class="n">vector</span>
</code></pre></div>
    
    <button class="codeblock__copy btn btn-outline-secondary border m-1 px-1 d-hover-only" title="Copy to clipboard"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#@copy"/></svg></span></button>
    <template class="codeblock__copied-template">
      <span class="small"><span class="icon baseline mr-1 text-success"><svg><use href="/static/icons.0c2040efe47b.svg#@check"/></svg></span>Copied!</span>
    </template>
    
  </div>
</div>
<p><code>gradient_descent()</code> takes four arguments:</p>
<ol>
<li><strong><code>gradient</code></strong> is the <a href="https://realpython.com/defining-your-own-python-function/">function</a> or any Python <a href="https://docs.python.org/3/reference/datamodel.html#emulating-callable-objects">callable object</a> that takes a vector and returns the gradient of the function you&rsquo;re trying to minimize.</li>
<li><strong><code>start</code></strong> is the point where the algorithm starts its search, given as a sequence (<a href="https://realpython.com/python-tuple/">tuple</a>, <a href="https://realpython.com/python-list/">list</a>, <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html">NumPy array</a>, and so on) or scalar (in the case of a one-dimensional problem).</li>
<li><strong><code>learn_rate</code></strong> is the learning rate that controls the magnitude of the vector update.</li>
<li><strong><code>n_iter</code></strong> is the number of iterations.</li>
</ol>
<p>This function does exactly what&rsquo;s described <a href="#intuition-behind-gradient-descent">above</a>: it takes a starting point (line 2), iteratively updates it according to the learning rate and the value of the gradient (lines 3 to 5), and finally returns the last position found.</p>
<p>Before you apply <code>gradient_descent()</code>, you can add another termination criterion:</p>
<div class="codeblock mb-3 w-100" aria-label="Code block">
  <div class="codeblock__header d-flex justify-content-between codeblock--blue">
    <span class="mr-2 noselect" aria-label="Language">Python</span>
    
    <div class="noselect">
      
    </div>
  </div>
  <div style="position: relative;">
    <div class="highlight python highlight--with-header"><pre><span></span><code><span class="hll"><span class="linenos"> 1</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span><span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span>
<span class="hll"><span class="linenos"> 4</span>    <span class="n">gradient</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">learn_rate</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="mf">1e-06</span>
</span><span class="linenos"> 5</span><span class="p">):</span>
<span class="linenos"> 6</span>    <span class="n">vector</span> <span class="o">=</span> <span class="n">start</span>
<span class="linenos"> 7</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
<span class="linenos"> 8</span>        <span class="n">diff</span> <span class="o">=</span> <span class="o">-</span><span class="n">learn_rate</span> <span class="o">*</span> <span class="n">gradient</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span>
<span class="hll"><span class="linenos"> 9</span>        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">tolerance</span><span class="p">):</span>
</span><span class="hll"><span class="linenos">10</span>            <span class="k">break</span>
</span><span class="linenos">11</span>        <span class="n">vector</span> <span class="o">+=</span> <span class="n">diff</span>
<span class="linenos">12</span>    <span class="k">return</span> <span class="n">vector</span>
</code></pre></div>
    
    <button class="codeblock__copy btn btn-outline-secondary border m-1 px-1 d-hover-only" title="Copy to clipboard"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#@copy"/></svg></span></button>
    <template class="codeblock__copied-template">
      <span class="small"><span class="icon baseline mr-1 text-success"><svg><use href="/static/icons.0c2040efe47b.svg#@check"/></svg></span>Copied!</span>
    </template>
    
  </div>
</div>
<p>You now have the additional parameter <code>tolerance</code> (line 4), which specifies the minimal allowed movement in each iteration. You&rsquo;ve also defined the default values for <code>tolerance</code> and <code>n_iter</code>, so you don&rsquo;t have to specify them each time you call <code>gradient_descent()</code>.</p>
<p>Lines 9 and 10 enable <code>gradient_descent()</code> to stop iterating and return the result before <code>n_iter</code> is reached if the vector update in the current iteration is less than or equal to <code>tolerance</code>. This often happens near the minimum, where gradients are usually very small. Unfortunately, it can also happen near a local minimum or a saddle point.</p>
<p>Line 9 uses the convenient NumPy functions <a href="https://numpy.org/doc/stable/reference/generated/numpy.all.html"><code>numpy.all()</code></a> and <a href="https://numpy.org/doc/stable/reference/generated/numpy.absolute.html"><code>numpy.abs()</code></a> to compare the <a href="https://realpython.com/python-absolute-value">absolute values</a> of <code>diff</code> and <code>tolerance</code> in a single statement. That&rsquo;s why you <code>import numpy</code> on line 1.</p>
<p>Now that you have the first version of <code>gradient_descent()</code>, it&rsquo;s time to test your function. You&rsquo;ll start with a small example and find the minimum of the function <a href="https://www.wolframalpha.com/input/?i=v**2">𝐶 = 𝑣²</a>.</p>
<p>This function has only one independent variable (𝑣), and its gradient is the derivative 2𝑣. It&rsquo;s a differentiable <a href="https://en.wikipedia.org/wiki/Convex_function">convex function</a>, and the analytical way to find its minimum is straightforward. However, in practice, analytical differentiation can be difficult or even impossible and is often approximated with <a href="https://en.wikipedia.org/wiki/Numerical_method">numerical methods</a>.</p>
<p>You need only one statement to test your gradient descent implementation:</p>
<div class="codeblock mb-3 w-100" aria-label="Code block">
  <div class="codeblock__header d-flex justify-content-between codeblock--blue">
    <span class="mr-2 noselect" aria-label="Language">Python</span>
    
    <div class="noselect">
      
        <span class="codeblock__output-toggle" title="Toggle prompts and output"><span class="icon baseline js-codeblock-output-on codeblock__header--icon-lower"><svg><use href="/static/icons.0c2040efe47b.svg#regular--rectangle-terminal"/></svg></span></span>
      
    </div>
  </div>
  <div style="position: relative;">
    <div class="highlight python repl highlight--with-header"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">gradient_descent</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">gradient</span><span class="o">=</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">v</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">learn_rate</span><span class="o">=</span><span class="mf">0.2</span>
<span class="gp">... </span><span class="p">)</span>
<span class="go">2.210739197207331e-06</span>
</code></pre></div>
    
    <button class="codeblock__copy btn btn-outline-secondary border m-1 px-1 d-hover-only" title="Copy to clipboard"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#@copy"/></svg></span></button>
    <template class="codeblock__copied-template">
      <span class="small"><span class="icon baseline mr-1 text-success"><svg><use href="/static/icons.0c2040efe47b.svg#@check"/></svg></span>Copied!</span>
    </template>
    
  </div>
</div>
<p>You use the <a href="https://realpython.com/python-lambda/">lambda function</a> <code>lambda v: 2 * v</code> to provide the gradient of 𝑣². You start from the value <code>10.0</code> and set the learning rate to <code>0.2</code>. You get a result that&rsquo;s very close to zero, which is the correct minimum.</p>
<p>The figure below shows the movement of the solution through the iterations:</p>
<figure class="js-lightbox"><a href="https://files.realpython.com/media/gd-1.25c5ef2aed4e.png" target="_blank"><img loading="lazy" class="img-fluid mx-auto d-block w-75" src="https://files.realpython.com/media/gd-1.25c5ef2aed4e.png" width="823" height="535" srcset="https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/gd-1.25c5ef2aed4e.png&amp;w=205&amp;sig=d534fb0dfc579f699ffb78ea8d4ca77bb178c67e 205w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/gd-1.25c5ef2aed4e.png&amp;w=274&amp;sig=feacabc50f1e92a68f6069293cfd05f5799fb8dd 274w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/gd-1.25c5ef2aed4e.png&amp;w=411&amp;sig=c135b59136f2f99643479031b92c697904096fff 411w, https://files.realpython.com/media/gd-1.25c5ef2aed4e.png 823w" sizes="(min-width: 1200px) 690px, (min-width: 780px) calc(-5vw + 669px), (min-width: 580px) 510px, calc(100vw - 30px)" alt="gda-perfect-updates" data-asset="3085"/></a></figure>

<p>You start from the rightmost green dot (𝑣 = 10) and move toward the minimum (𝑣 = 0). The updates are larger at first because the value of the gradient (and slope) is higher. As you approach the minimum, they become lower.</p>
<div><div class="rounded border border-light" style="display:block;position:relative;"> <div style="display:block;width:100%;padding-top:12.5%;"></div> <div class="rpad rounded border" data-unit="8x1" style="position:absolute;left:0;top:0;right:0;bottom:0;overflow:hidden;"> </div></div><a class="small text-muted" href="/account/join/" rel="nofollow"> <span class="icon baseline mr-1"><svg><use href="/static/icons.0c2040efe47b.svg#solid--circle-info"></use></svg></span>Remove ads</a></div></section><section class="section3"><h3 id="learning-rate-impact">Learning Rate Impact<a class="headerlink" href="#learning-rate-impact" title="Permanent link"></a></h3>
<p>The learning rate is a very important parameter of the algorithm. Different learning rate values can significantly affect the behavior of gradient descent. Consider the previous example, but with a learning rate of 0.8 instead of 0.2:</p>
<div class="codeblock mb-3 w-100" aria-label="Code block">
  <div class="codeblock__header d-flex justify-content-between codeblock--blue">
    <span class="mr-2 noselect" aria-label="Language">Python</span>
    
    <div class="noselect">
      
        <span class="codeblock__output-toggle" title="Toggle prompts and output"><span class="icon baseline js-codeblock-output-on codeblock__header--icon-lower"><svg><use href="/static/icons.0c2040efe47b.svg#regular--rectangle-terminal"/></svg></span></span>
      
    </div>
  </div>
  <div style="position: relative;">
    <div class="highlight python repl highlight--with-header"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">gradient_descent</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">gradient</span><span class="o">=</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">v</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">learn_rate</span><span class="o">=</span><span class="mf">0.8</span>
<span class="gp">... </span><span class="p">)</span>
<span class="go">-4.77519666596786e-07</span>
</code></pre></div>
    
    <button class="codeblock__copy btn btn-outline-secondary border m-1 px-1 d-hover-only" title="Copy to clipboard"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#@copy"/></svg></span></button>
    <template class="codeblock__copied-template">
      <span class="small"><span class="icon baseline mr-1 text-success"><svg><use href="/static/icons.0c2040efe47b.svg#@check"/></svg></span>Copied!</span>
    </template>
    
  </div>
</div>
<p>You get another solution that&rsquo;s very close to zero, but the internal behavior of the algorithm is different. This is what happens with the value of 𝑣 through the iterations:</p>
<figure class="js-lightbox"><a href="https://files.realpython.com/media/gd-3.ff9f92989807.png" target="_blank"><img loading="lazy" class="img-fluid mx-auto d-block w-75" src="https://files.realpython.com/media/gd-3.ff9f92989807.png" width="823" height="535" srcset="https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/gd-3.ff9f92989807.png&amp;w=205&amp;sig=a38895ba0fd400cb7595e0cd5b3d935f93c0fe56 205w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/gd-3.ff9f92989807.png&amp;w=274&amp;sig=98b38f7dd55ba5b5ac6d368270d1fa5978c9a709 274w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/gd-3.ff9f92989807.png&amp;w=411&amp;sig=b88f99d48d80471c984ed8f7f3812d2500ad89d5 411w, https://files.realpython.com/media/gd-3.ff9f92989807.png 823w" sizes="(min-width: 1200px) 690px, (min-width: 780px) calc(-5vw + 669px), (min-width: 580px) 510px, calc(100vw - 30px)" alt="gda-large-learning-rate" data-asset="3086"/></a></figure>

<p>In this case, you again start with 𝑣 = 10, but because of the high learning rate, you get a large change in 𝑣 that passes to the other side of the optimum and becomes −6. It crosses zero a few more times before settling near it.</p>
<p>Small learning rates can result in very slow convergence. If the number of iterations is limited, then the algorithm may return before the minimum is found. Otherwise, the whole process might take an unacceptably large amount of time. To illustrate this, run <code>gradient_descent()</code> again, this time with a much smaller learning rate of 0.005:</p>
<div class="codeblock mb-3 w-100" aria-label="Code block">
  <div class="codeblock__header d-flex justify-content-between codeblock--blue">
    <span class="mr-2 noselect" aria-label="Language">Python</span>
    
    <div class="noselect">
      
        <span class="codeblock__output-toggle" title="Toggle prompts and output"><span class="icon baseline js-codeblock-output-on codeblock__header--icon-lower"><svg><use href="/static/icons.0c2040efe47b.svg#regular--rectangle-terminal"/></svg></span></span>
      
    </div>
  </div>
  <div style="position: relative;">
    <div class="highlight python repl highlight--with-header"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">gradient_descent</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">gradient</span><span class="o">=</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">v</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">learn_rate</span><span class="o">=</span><span class="mf">0.005</span>
<span class="gp">... </span><span class="p">)</span>
<span class="go">6.050060671375367</span>
</code></pre></div>
    
    <button class="codeblock__copy btn btn-outline-secondary border m-1 px-1 d-hover-only" title="Copy to clipboard"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#@copy"/></svg></span></button>
    <template class="codeblock__copied-template">
      <span class="small"><span class="icon baseline mr-1 text-success"><svg><use href="/static/icons.0c2040efe47b.svg#@check"/></svg></span>Copied!</span>
    </template>
    
  </div>
</div>
<p>The result is now <code>6.05</code>, which is nowhere near the true minimum of zero. This is because the changes in the vector are very small due to the small learning rate:</p>
<figure class="js-lightbox"><a href="https://files.realpython.com/media/gd-4.9a5c436570fd.png" target="_blank"><img loading="lazy" class="img-fluid mx-auto d-block w-75" src="https://files.realpython.com/media/gd-4.9a5c436570fd.png" width="823" height="535" srcset="https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/gd-4.9a5c436570fd.png&amp;w=205&amp;sig=5cbd7e82230ff71fceb98d90a648cf84547564bb 205w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/gd-4.9a5c436570fd.png&amp;w=274&amp;sig=93fbc54b683d2fe34704c6dac86e425808b583ca 274w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/gd-4.9a5c436570fd.png&amp;w=411&amp;sig=89e240ea660ebd2323dbe3cffdf20a0a0c71a934 411w, https://files.realpython.com/media/gd-4.9a5c436570fd.png 823w" sizes="(min-width: 1200px) 690px, (min-width: 780px) calc(-5vw + 669px), (min-width: 580px) 510px, calc(100vw - 30px)" alt="gda-small-learning-rate" data-asset="3087"/></a></figure>

<p>The search process starts at 𝑣 = 10 as before, but it can&rsquo;t reach zero in fifty iterations. However, with a hundred iterations, the error will be much smaller, and with a thousand iterations, you&rsquo;ll be very close to zero:</p>
<div class="codeblock mb-3 w-100" aria-label="Code block">
  <div class="codeblock__header d-flex justify-content-between codeblock--blue">
    <span class="mr-2 noselect" aria-label="Language">Python</span>
    
    <div class="noselect">
      
        <span class="codeblock__output-toggle" title="Toggle prompts and output"><span class="icon baseline js-codeblock-output-on codeblock__header--icon-lower"><svg><use href="/static/icons.0c2040efe47b.svg#regular--rectangle-terminal"/></svg></span></span>
      
    </div>
  </div>
  <div style="position: relative;">
    <div class="highlight python repl highlight--with-header"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">gradient_descent</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">gradient</span><span class="o">=</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">v</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">learn_rate</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">n_iter</span><span class="o">=</span><span class="mi">100</span>
<span class="gp">... </span><span class="p">)</span>
<span class="go">3.660323412732294</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gradient_descent</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">gradient</span><span class="o">=</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">v</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">learn_rate</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">n_iter</span><span class="o">=</span><span class="mi">1000</span>
<span class="gp">... </span><span class="p">)</span>
<span class="go">0.0004317124741065828</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gradient_descent</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">gradient</span><span class="o">=</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">v</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">learn_rate</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span>
<span class="gp">... </span><span class="p">)</span>
<span class="go">9.952518849647663e-05</span>
</code></pre></div>
    
    <button class="codeblock__copy btn btn-outline-secondary border m-1 px-1 d-hover-only" title="Copy to clipboard"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#@copy"/></svg></span></button>
    <template class="codeblock__copied-template">
      <span class="small"><span class="icon baseline mr-1 text-success"><svg><use href="/static/icons.0c2040efe47b.svg#@check"/></svg></span>Copied!</span>
    </template>
    
  </div>
</div>
<p>Nonconvex functions might have local minima or saddle points where the algorithm can get trapped. In such situations, your choice of learning rate or starting point can make the difference between finding a local minimum and finding the global minimum.</p>
<p>Consider the function <a href="https://www.wolframalpha.com/input/?i=v**4+-+5+*+v**2+-+3+*+v">𝑣⁴ - 5𝑣² - 3𝑣</a>. It has a global minimum in 𝑣 ≈ 1.7 and a local minimum in 𝑣 ≈ −1.42. The gradient of this function is 4𝑣³ − 10𝑣 − 3. Let&rsquo;s see how <code>gradient_descent()</code> works here:</p>
<div class="codeblock mb-3 w-100" aria-label="Code block">
  <div class="codeblock__header d-flex justify-content-between codeblock--blue">
    <span class="mr-2 noselect" aria-label="Language">Python</span>
    
    <div class="noselect">
      
        <span class="codeblock__output-toggle" title="Toggle prompts and output"><span class="icon baseline js-codeblock-output-on codeblock__header--icon-lower"><svg><use href="/static/icons.0c2040efe47b.svg#regular--rectangle-terminal"/></svg></span></span>
      
    </div>
  </div>
  <div style="position: relative;">
    <div class="highlight python repl highlight--with-header"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">gradient_descent</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">gradient</span><span class="o">=</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">v</span><span class="o">**</span><span class="mi">3</span> <span class="o">-</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">v</span> <span class="o">-</span> <span class="mi">3</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">learn_rate</span><span class="o">=</span><span class="mf">0.2</span>
<span class="gp">... </span><span class="p">)</span>
<span class="go">-1.4207567437458342</span>
</code></pre></div>
    
    <button class="codeblock__copy btn btn-outline-secondary border m-1 px-1 d-hover-only" title="Copy to clipboard"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#@copy"/></svg></span></button>
    <template class="codeblock__copied-template">
      <span class="small"><span class="icon baseline mr-1 text-success"><svg><use href="/static/icons.0c2040efe47b.svg#@check"/></svg></span>Copied!</span>
    </template>
    
  </div>
</div>
<p>You started at zero this time, and the algorithm ended near the local minimum. Here&rsquo;s what happened under the hood:</p>
<figure class="js-lightbox"><a href="https://files.realpython.com/media/gd-7.67e03e9337db.png" target="_blank"><img loading="lazy" class="img-fluid mx-auto d-block w-75" src="https://files.realpython.com/media/gd-7.67e03e9337db.png" width="823" height="535" srcset="https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/gd-7.67e03e9337db.png&amp;w=205&amp;sig=33acac905874918d20038031d438706347461bd2 205w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/gd-7.67e03e9337db.png&amp;w=274&amp;sig=0913380f3369508d02b8eb4876225c03013f5683 274w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/gd-7.67e03e9337db.png&amp;w=411&amp;sig=ffe323b6d16b71f8876dd246f727c39a4051d3ed 411w, https://files.realpython.com/media/gd-7.67e03e9337db.png 823w" sizes="(min-width: 1200px) 690px, (min-width: 780px) calc(-5vw + 669px), (min-width: 580px) 510px, calc(100vw - 30px)" alt="gda-local-minimum" data-asset="3088"/></a></figure>

<p>During the first two iterations, your vector was moving toward the global minimum, but then it crossed to the opposite side and stayed trapped in the local minimum. You can prevent this with a smaller learning rate:</p>
<div class="codeblock mb-3 w-100" aria-label="Code block">
  <div class="codeblock__header d-flex justify-content-between codeblock--blue">
    <span class="mr-2 noselect" aria-label="Language">Python</span>
    
    <div class="noselect">
      
        <span class="codeblock__output-toggle" title="Toggle prompts and output"><span class="icon baseline js-codeblock-output-on codeblock__header--icon-lower"><svg><use href="/static/icons.0c2040efe47b.svg#regular--rectangle-terminal"/></svg></span></span>
      
    </div>
  </div>
  <div style="position: relative;">
    <div class="highlight python repl highlight--with-header"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">gradient_descent</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">gradient</span><span class="o">=</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">v</span><span class="o">**</span><span class="mi">3</span> <span class="o">-</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">v</span> <span class="o">-</span> <span class="mi">3</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">learn_rate</span><span class="o">=</span><span class="mf">0.1</span>
<span class="gp">... </span><span class="p">)</span>
<span class="go">1.285401330315467</span>
</code></pre></div>
    
    <button class="codeblock__copy btn btn-outline-secondary border m-1 px-1 d-hover-only" title="Copy to clipboard"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#@copy"/></svg></span></button>
    <template class="codeblock__copied-template">
      <span class="small"><span class="icon baseline mr-1 text-success"><svg><use href="/static/icons.0c2040efe47b.svg#@check"/></svg></span>Copied!</span>
    </template>
    
  </div>
</div>
<p>When you decrease the learning rate from <code>0.2</code> to <code>0.1</code>, you get a solution very close to the global minimum. Remember that gradient descent is an approximate method. This time, you avoid the jump to the other side:</p>
<figure class="js-lightbox"><a href="https://files.realpython.com/media/gd-8.f055cad0b634.png" target="_blank"><img loading="lazy" class="img-fluid mx-auto d-block w-75" src="https://files.realpython.com/media/gd-8.f055cad0b634.png" width="823" height="535" srcset="https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/gd-8.f055cad0b634.png&amp;w=205&amp;sig=e21a1edbc542b1ed9a039268b85d083b4a94408a 205w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/gd-8.f055cad0b634.png&amp;w=274&amp;sig=3604be5c7c32e36c6dfa1349afa70b35dd541404 274w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/gd-8.f055cad0b634.png&amp;w=411&amp;sig=740379d1deca33330b1cbbda181d6028ddafc7fb 411w, https://files.realpython.com/media/gd-8.f055cad0b634.png 823w" sizes="(min-width: 1200px) 690px, (min-width: 780px) calc(-5vw + 669px), (min-width: 580px) 510px, calc(100vw - 30px)" alt="gda-global-minimum" data-asset="3089"/></a></figure>

<p>A lower learning rate prevents the vector from making large jumps, and in this case, the vector remains closer to the global optimum.</p>
<p>Adjusting the learning rate is tricky. You can&rsquo;t know the best value in advance. There are many techniques and heuristics that try to help with this. In addition, machine learning practitioners often tune the learning rate during model selection and evaluation.</p>
<p>Besides the learning rate, the starting point can affect the solution significantly, especially with nonconvex functions.</p>
<div><div class="rounded border border-light" style="display:block;position:relative;"> <div style="display:block;width:100%;padding-top:12.5%;"></div> <div class="rpad rounded border" data-unit="8x1" style="position:absolute;left:0;top:0;right:0;bottom:0;overflow:hidden;"> </div></div><a class="small text-muted" href="/account/join/" rel="nofollow"> <span class="icon baseline mr-1"><svg><use href="/static/icons.0c2040efe47b.svg#solid--circle-info"></use></svg></span>Remove ads</a></div></section></section><section class="section2"><h2 id="application-of-the-gradient-descent-algorithm">Application of the Gradient Descent Algorithm<a class="headerlink" href="#application-of-the-gradient-descent-algorithm" title="Permanent link"></a></h2>
<p>In this section, you&rsquo;ll see two short examples of using gradient descent. You&rsquo;ll also learn that it can be used in real-life machine learning problems like linear regression. In the second case, you&rsquo;ll need to modify the code of <code>gradient_descent()</code> because you need the data from the observations to calculate the gradient.</p>
<section class="section3"><h3 id="short-examples">Short Examples<a class="headerlink" href="#short-examples" title="Permanent link"></a></h3>
<p>First, you&rsquo;ll apply <code>gradient_descent()</code> to another one-dimensional problem. Take the function <a href="https://www.wolframalpha.com/input/?i=v+-+log%28v%29">𝑣 − log(𝑣)</a>. The gradient of this function is 1 − 1/𝑣. With this information, you can find its minimum:</p>
<div class="codeblock mb-3 w-100" aria-label="Code block">
  <div class="codeblock__header d-flex justify-content-between codeblock--blue">
    <span class="mr-2 noselect" aria-label="Language">Python</span>
    
    <div class="noselect">
      
        <span class="codeblock__output-toggle" title="Toggle prompts and output"><span class="icon baseline js-codeblock-output-on codeblock__header--icon-lower"><svg><use href="/static/icons.0c2040efe47b.svg#regular--rectangle-terminal"/></svg></span></span>
      
    </div>
  </div>
  <div style="position: relative;">
    <div class="highlight python repl highlight--with-header"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">gradient_descent</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">gradient</span><span class="o">=</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">v</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">learn_rate</span><span class="o">=</span><span class="mf">0.5</span>
<span class="gp">... </span><span class="p">)</span>
<span class="go">1.0000011077232125</span>
</code></pre></div>
    
    <button class="codeblock__copy btn btn-outline-secondary border m-1 px-1 d-hover-only" title="Copy to clipboard"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#@copy"/></svg></span></button>
    <template class="codeblock__copied-template">
      <span class="small"><span class="icon baseline mr-1 text-success"><svg><use href="/static/icons.0c2040efe47b.svg#@check"/></svg></span>Copied!</span>
    </template>
    
  </div>
</div>
<p>With the provided set of arguments, <code>gradient_descent()</code> correctly calculates that this function has the minimum in 𝑣 = 1. You can try it with other values for the learning rate and starting point.</p>
<p>You can also use <code>gradient_descent()</code> with functions of more than one variable. The application is the same, but you need to provide the gradient and starting points as vectors or arrays. For example, you can find the minimum of the function <a href="https://www.wolframalpha.com/input/?i=v_1**2+%2B+v_2**4">𝑣₁² + 𝑣₂⁴</a> that has the gradient vector (2𝑣₁, 4𝑣₂³):</p>
<div class="codeblock mb-3 w-100" aria-label="Code block">
  <div class="codeblock__header d-flex justify-content-between codeblock--blue">
    <span class="mr-2 noselect" aria-label="Language">Python</span>
    
    <div class="noselect">
      
        <span class="codeblock__output-toggle" title="Toggle prompts and output"><span class="icon baseline js-codeblock-output-on codeblock__header--icon-lower"><svg><use href="/static/icons.0c2040efe47b.svg#regular--rectangle-terminal"/></svg></span></span>
      
    </div>
  </div>
  <div style="position: relative;">
    <div class="highlight python repl highlight--with-header"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">gradient_descent</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">gradient</span><span class="o">=</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span> <span class="o">*</span> <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">3</span><span class="p">]),</span>
<span class="gp">... </span>    <span class="n">start</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]),</span> <span class="n">learn_rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="mf">1e-08</span>
<span class="gp">... </span><span class="p">)</span>
<span class="go">array([8.08281277e-12, 9.75207120e-02])</span>
</code></pre></div>
    
    <button class="codeblock__copy btn btn-outline-secondary border m-1 px-1 d-hover-only" title="Copy to clipboard"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#@copy"/></svg></span></button>
    <template class="codeblock__copied-template">
      <span class="small"><span class="icon baseline mr-1 text-success"><svg><use href="/static/icons.0c2040efe47b.svg#@check"/></svg></span>Copied!</span>
    </template>
    
  </div>
</div>
<p>In this case, your gradient function returns an array, and the start value is an array, so you get an array as the result. The resulting values are almost equal to zero, so you can say that <code>gradient_descent()</code> correctly found that the minimum of this function is at 𝑣₁ = 𝑣₂ = 0.</p>
</section><section class="section3"><h3 id="ordinary-least-squares">Ordinary Least Squares<a class="headerlink" href="#ordinary-least-squares" title="Permanent link"></a></h3>
<p>As you&rsquo;ve already learned, linear regression and the <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares">ordinary least squares method</a> start with the observed values of the inputs 𝐱 = (𝑥₁, …, 𝑥ᵣ) and outputs 𝑦. They define a linear function 𝑓(𝐱) = 𝑏₀ + 𝑏₁𝑥₁ + ⋯ + 𝑏ᵣ𝑥ᵣ, which is as close as possible to 𝑦.</p>
<p>This is an optimization problem. It finds the values of weights 𝑏₀, 𝑏₁, …, 𝑏ᵣ that minimize the sum of squared residuals SSR = Σᵢ(𝑦ᵢ − 𝑓(𝐱ᵢ))² or the mean squared error MSE = SSR / 𝑛. Here, 𝑛 is the total number of observations and 𝑖 = 1, …, 𝑛.</p>
<p>You can also use the cost function 𝐶 = SSR / (2𝑛), which is mathematically more convenient than SSR or MSE.</p>
<p>The most basic form of linear regression is <a href="https://realpython.com/linear-regression-in-python/#simple-linear-regression">simple linear regression</a>. It has only one set of inputs 𝑥 and two weights: 𝑏₀ and 𝑏₁. The equation of the regression line is 𝑓(𝑥) = 𝑏₀ + 𝑏₁𝑥. Although the optimal values of 𝑏₀ and 𝑏₁ can be <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares#Simple_linear_regression_model">calculated analytically</a>, you&rsquo;ll use gradient descent to determine them.</p>
<p>First, you need calculus to find the gradient of the cost function 𝐶 = Σᵢ(𝑦ᵢ − 𝑏₀ − 𝑏₁𝑥ᵢ)² / (2𝑛). Since you have two decision variables, 𝑏₀ and 𝑏₁, the gradient ∇𝐶 is a vector with two components:</p>
<ol>
<li>∂𝐶/∂𝑏₀ = (1/𝑛) Σᵢ(𝑏₀ + 𝑏₁𝑥ᵢ − 𝑦ᵢ) = mean(𝑏₀ + 𝑏₁𝑥ᵢ − 𝑦ᵢ)</li>
<li>∂𝐶/∂𝑏₁ = (1/𝑛) Σᵢ(𝑏₀ + 𝑏₁𝑥ᵢ − 𝑦ᵢ) 𝑥ᵢ = mean((𝑏₀ + 𝑏₁𝑥ᵢ − 𝑦ᵢ) 𝑥ᵢ)</li>
</ol>
<p>You need the values of 𝑥 and 𝑦 to calculate the gradient of this cost function. Your gradient function will have as inputs not only 𝑏₀ and 𝑏₁ but also 𝑥 and 𝑦. This is how it might look:</p>
<div class="codeblock mb-3 w-100" aria-label="Code block">
  <div class="codeblock__header d-flex justify-content-between codeblock--blue">
    <span class="mr-2 noselect" aria-label="Language">Python</span>
    
    <div class="noselect">
      
    </div>
  </div>
  <div style="position: relative;">
    <div class="highlight python highlight--with-header"><pre><span></span><code><span class="k">def</span> <span class="nf">ssr_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span>
    <span class="k">return</span> <span class="n">res</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="p">(</span><span class="n">res</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>  <span class="c1"># .mean() is a method of np.ndarray</span>
</code></pre></div>
    
    <button class="codeblock__copy btn btn-outline-secondary border m-1 px-1 d-hover-only" title="Copy to clipboard"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#@copy"/></svg></span></button>
    <template class="codeblock__copied-template">
      <span class="small"><span class="icon baseline mr-1 text-success"><svg><use href="/static/icons.0c2040efe47b.svg#@check"/></svg></span>Copied!</span>
    </template>
    
  </div>
</div>
<p><code>ssr_gradient()</code> takes the arrays <code>x</code> and <code>y</code>, which contain the observation inputs and outputs, and the array <code>b</code> that holds the current values of the decision variables 𝑏₀ and 𝑏₁. This function first calculates the array of the residuals for each observation (<code>res</code>) and then returns the pair of values of ∂𝐶/∂𝑏₀ and ∂𝐶/∂𝑏₁.</p>
<p>In this example, you can use the convenient NumPy method <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.mean.html"><code>ndarray.mean()</code></a> since you pass NumPy arrays as the arguments.</p>
<p><code>gradient_descent()</code> needs two small adjustments:</p>
<ol>
<li>Add <code>x</code> and <code>y</code> as the parameters of <code>gradient_descent()</code> on line 4.</li>
<li>Provide <code>x</code> and <code>y</code> to the gradient function and make sure you convert your gradient tuple to a NumPy array on line 8.</li>
</ol>
<p>Here&rsquo;s how <code>gradient_descent()</code> looks after these changes:</p>
<div class="codeblock mb-3 w-100" aria-label="Code block">
  <div class="codeblock__header d-flex justify-content-between codeblock--blue">
    <span class="mr-2 noselect" aria-label="Language">Python</span>
    
    <div class="noselect">
      
    </div>
  </div>
  <div style="position: relative;">
    <div class="highlight python highlight--with-header"><pre><span></span><code><span class="linenos"> 1</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span>
<span class="hll"><span class="linenos"> 4</span>    <span class="n">gradient</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">learn_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="mf">1e-06</span>
</span><span class="linenos"> 5</span><span class="p">):</span>
<span class="linenos"> 6</span>    <span class="n">vector</span> <span class="o">=</span> <span class="n">start</span>
<span class="linenos"> 7</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
<span class="hll"><span class="linenos"> 8</span>        <span class="n">diff</span> <span class="o">=</span> <span class="o">-</span><span class="n">learn_rate</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">vector</span><span class="p">))</span>
</span><span class="linenos"> 9</span>        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">tolerance</span><span class="p">):</span>
<span class="linenos">10</span>            <span class="k">break</span>
<span class="linenos">11</span>        <span class="n">vector</span> <span class="o">+=</span> <span class="n">diff</span>
<span class="linenos">12</span>    <span class="k">return</span> <span class="n">vector</span>
</code></pre></div>
    
    <button class="codeblock__copy btn btn-outline-secondary border m-1 px-1 d-hover-only" title="Copy to clipboard"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#@copy"/></svg></span></button>
    <template class="codeblock__copied-template">
      <span class="small"><span class="icon baseline mr-1 text-success"><svg><use href="/static/icons.0c2040efe47b.svg#@check"/></svg></span>Copied!</span>
    </template>
    
  </div>
</div>
<p><code>gradient_descent()</code> now accepts the observation inputs <code>x</code> and outputs <code>y</code> and can use them to calculate the gradient. Converting the output of <code>gradient(x, y, vector)</code> to a NumPy array enables elementwise multiplication of the gradient elements by the learning rate, which isn&rsquo;t necessary in the case of a single-variable function.</p>
<p>Now apply your new version of <code>gradient_descent()</code> to find the regression line for some arbitrary values of <code>x</code> and <code>y</code>:</p>
<div class="codeblock mb-3 w-100" aria-label="Code block">
  <div class="codeblock__header d-flex justify-content-between codeblock--blue">
    <span class="mr-2 noselect" aria-label="Language">Python</span>
    
    <div class="noselect">
      
        <span class="codeblock__output-toggle" title="Toggle prompts and output"><span class="icon baseline js-codeblock-output-on codeblock__header--icon-lower"><svg><use href="/static/icons.0c2040efe47b.svg#regular--rectangle-terminal"/></svg></span></span>
      
    </div>
  </div>
  <div style="position: relative;">
    <div class="highlight python repl highlight--with-header"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">55</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">38</span><span class="p">])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">gradient_descent</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">ssr_gradient</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">learn_rate</span><span class="o">=</span><span class="mf">0.0008</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">n_iter</span><span class="o">=</span><span class="mi">100_000</span>
<span class="gp">... </span><span class="p">)</span>
<span class="go">array([5.62822349, 0.54012867])</span>
</code></pre></div>
    
    <button class="codeblock__copy btn btn-outline-secondary border m-1 px-1 d-hover-only" title="Copy to clipboard"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#@copy"/></svg></span></button>
    <template class="codeblock__copied-template">
      <span class="small"><span class="icon baseline mr-1 text-success"><svg><use href="/static/icons.0c2040efe47b.svg#@check"/></svg></span>Copied!</span>
    </template>
    
  </div>
</div>
<p>The result is an array with two values that correspond to the decision variables: 𝑏₀ = 5.63 and 𝑏₁ = 0.54. The best regression line is 𝑓(𝑥) = 5.63 + 0.54𝑥. As in the previous examples, this result heavily depends on the learning rate. You might not get such a good result with too low or too high of a learning rate.</p>
<p>This example isn&rsquo;t entirely random&ndash;it&rsquo;s taken from the tutorial <a href="https://realpython.com/linear-regression-in-python/#simple-linear-regression-with-scikit-learn">Linear Regression in Python</a>. The good news is that you&rsquo;ve obtained almost the same result as the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">linear regressor from scikit-learn</a>. The data and regression results are visualized in the section <a href="https://realpython.com/linear-regression-in-python/#simple-linear-regression">Simple Linear Regression</a>.</p>
<div><div class="rounded border border-light" style="display:block;position:relative;"> <div style="display:block;width:100%;padding-top:12.5%;"></div> <div class="rpad rounded border" data-unit="8x1" style="position:absolute;left:0;top:0;right:0;bottom:0;overflow:hidden;"> </div></div><a class="small text-muted" href="/account/join/" rel="nofollow"> <span class="icon baseline mr-1"><svg><use href="/static/icons.0c2040efe47b.svg#solid--circle-info"></use></svg></span>Remove ads</a></div></section><section class="section3"><h3 id="improvement-of-the-code">Improvement of the Code<a class="headerlink" href="#improvement-of-the-code" title="Permanent link"></a></h3>
<p>You can make <code>gradient_descent()</code> more robust, comprehensive, and better-looking without modifying its core functionality:</p>
<div class="codeblock mb-3 w-100" aria-label="Code block">
  <div class="codeblock__header d-flex justify-content-between codeblock--blue">
    <span class="mr-2 noselect" aria-label="Language">Python</span>
    
    <div class="noselect">
      
    </div>
  </div>
  <div style="position: relative;">
    <div class="highlight python highlight--with-header"><pre><span></span><code><span class="linenos"> 1</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span>
<span class="linenos"> 4</span>    <span class="n">gradient</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">learn_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">,</span>
<span class="linenos"> 5</span>    <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float64&quot;</span>
<span class="linenos"> 6</span><span class="p">):</span>
<span class="linenos"> 7</span>    <span class="c1"># Checking if the gradient is callable</span>
<span class="linenos"> 8</span>    <span class="k">if</span> <span class="ow">not</span> <span class="nb">callable</span><span class="p">(</span><span class="n">gradient</span><span class="p">):</span>
<span class="linenos"> 9</span>        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;&#39;gradient&#39; must be callable&quot;</span><span class="p">)</span>
<span class="linenos">10</span>
<span class="linenos">11</span>    <span class="c1"># Setting up the data type for NumPy arrays</span>
<span class="linenos">12</span>    <span class="n">dtype_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
<span class="linenos">13</span>
<span class="linenos">14</span>    <span class="c1"># Converting x and y to NumPy arrays</span>
<span class="linenos">15</span>    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">)</span>
<span class="linenos">16</span>    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
<span class="linenos">17</span>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;x&#39; and &#39;y&#39; lengths do not match&quot;</span><span class="p">)</span>
<span class="linenos">18</span>
<span class="linenos">19</span>    <span class="c1"># Initializing the values of the variables</span>
<span class="linenos">20</span>    <span class="n">vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">)</span>
<span class="linenos">21</span>
<span class="linenos">22</span>    <span class="c1"># Setting up and checking the learning rate</span>
<span class="linenos">23</span>    <span class="n">learn_rate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">learn_rate</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">)</span>
<span class="linenos">24</span>    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">learn_rate</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
<span class="linenos">25</span>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;learn_rate&#39; must be greater than zero&quot;</span><span class="p">)</span>
<span class="linenos">26</span>
<span class="linenos">27</span>    <span class="c1"># Setting up and checking the maximal number of iterations</span>
<span class="linenos">28</span>    <span class="n">n_iter</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_iter</span><span class="p">)</span>
<span class="linenos">29</span>    <span class="k">if</span> <span class="n">n_iter</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
<span class="linenos">30</span>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;n_iter&#39; must be greater than zero&quot;</span><span class="p">)</span>
<span class="linenos">31</span>
<span class="linenos">32</span>    <span class="c1"># Setting up and checking the tolerance</span>
<span class="linenos">33</span>    <span class="n">tolerance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">tolerance</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">)</span>
<span class="linenos">34</span>    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">tolerance</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
<span class="linenos">35</span>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;tolerance&#39; must be greater than zero&quot;</span><span class="p">)</span>
<span class="linenos">36</span>
<span class="linenos">37</span>    <span class="c1"># Performing the gradient descent loop</span>
<span class="linenos">38</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
<span class="linenos">39</span>        <span class="c1"># Recalculating the difference</span>
<span class="linenos">40</span>        <span class="n">diff</span> <span class="o">=</span> <span class="o">-</span><span class="n">learn_rate</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">vector</span><span class="p">),</span> <span class="n">dtype_</span><span class="p">)</span>
<span class="linenos">41</span>
<span class="linenos">42</span>        <span class="c1"># Checking if the absolute difference is small enough</span>
<span class="linenos">43</span>        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">tolerance</span><span class="p">):</span>
<span class="linenos">44</span>            <span class="k">break</span>
<span class="linenos">45</span>
<span class="linenos">46</span>        <span class="c1"># Updating the values of the variables</span>
<span class="linenos">47</span>        <span class="n">vector</span> <span class="o">+=</span> <span class="n">diff</span>
<span class="linenos">48</span>
<span class="linenos">49</span>    <span class="k">return</span> <span class="n">vector</span> <span class="k">if</span> <span class="n">vector</span><span class="o">.</span><span class="n">shape</span> <span class="k">else</span> <span class="n">vector</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</code></pre></div>
    
    <button class="codeblock__copy btn btn-outline-secondary border m-1 px-1 d-hover-only" title="Copy to clipboard"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#@copy"/></svg></span></button>
    <template class="codeblock__copied-template">
      <span class="small"><span class="icon baseline mr-1 text-success"><svg><use href="/static/icons.0c2040efe47b.svg#@check"/></svg></span>Copied!</span>
    </template>
    
  </div>
</div>
<p><code>gradient_descent()</code> now accepts an additional <code>dtype</code> parameter that defines the data type of NumPy arrays inside the function. For more information about NumPy types, see the <a href="https://numpy.org/doc/stable/user/basics.types.html">official documentation on data types</a>.</p>
<p>In most applications, you won&rsquo;t notice a difference between 32-bit and 64-bit floating-point numbers, but when you work with big datasets, this might significantly affect memory use and maybe even <a href="https://stackoverflow.com/questions/15340781/python-numpy-data-types-performance">processing speed</a>. For example, although NumPy uses 64-bit floats by default, <a href="https://www.tensorflow.org/guide/tensor">TensorFlow often uses 32-bit decimal numbers</a>.</p>
<p>In addition to considering data types, the code above introduces a few modifications related to type checking and ensuring the use of NumPy capabilities:</p>
<ul>
<li>
<p><strong>Lines 8 and 9</strong> check if <code>gradient</code> is a Python callable object and whether it can be used as a function. If not, then the function will raise a <a href="https://docs.python.org/3/library/exceptions.html#TypeError"><code>TypeError</code></a>.</p>
</li>
<li>
<p><strong>Line 12</strong> sets an instance of <a href="https://numpy.org/doc/stable/reference/generated/numpy.dtype.html"><code>numpy.dtype</code></a>, which will be used as the data type for all arrays throughout the function.</p>
</li>
<li>
<p><strong>Line 15</strong> takes the arguments <code>x</code> and <code>y</code> and produces NumPy arrays with the desired data type. The arguments <code>x</code> and <code>y</code> can be lists, tuples, arrays, or other sequences.</p>
</li>
<li>
<p><strong>Lines 16 and 17</strong> compare the sizes of <code>x</code> and <code>y</code>. This is useful because you want to be sure that both arrays have the same number of observations. If they don&rsquo;t, then the function will raise a <a href="https://docs.python.org/3/library/exceptions.html#ValueError"><code>ValueError</code></a>.</p>
</li>
<li>
<p><strong>Line 20</strong> converts the argument <code>start</code> to a NumPy array. This is an interesting trick: if <code>start</code> is a Python scalar, then it&rsquo;ll be transformed into a corresponding NumPy object (an array with one item and zero dimensions). If you pass a sequence, then it&rsquo;ll become a regular NumPy array with the same number of elements.</p>
</li>
<li>
<p><strong>Line 23</strong> does the same thing with the learning rate. This can be very useful because it enables you to specify different learning rates for each decision variable by passing a list, tuple, or NumPy array to <code>gradient_descent()</code>.</p>
</li>
<li>
<p><strong>Lines 24 and 25</strong> check if the learning rate value (or values for all variables) is greater than zero.</p>
</li>
<li>
<p><strong>Lines 28 to 35</strong> similarly set <code>n_iter</code> and <code>tolerance</code> and check that they are greater than zero.</p>
</li>
<li>
<p><strong>Lines 38 to 47</strong> are almost the same as before. The only difference is the type of the gradient array on line 40.</p>
</li>
<li>
<p><strong>Line 49</strong> conveniently returns the resulting array if you have several decision variables or a Python scalar if you have a single variable.</p>
</li>
</ul>
<p>Your <code>gradient_descent()</code> is now finished. Feel free to add some additional capabilities or polishing. The next step of this tutorial is to use what you&rsquo;ve learned so far to implement the stochastic version of gradient descent.</p>
</section></section><section class="section2"><h2 id="stochastic-gradient-descent-algorithms">Stochastic Gradient Descent Algorithms<a class="headerlink" href="#stochastic-gradient-descent-algorithms" title="Permanent link"></a></h2>
<p><strong>Stochastic gradient descent algorithms</strong> are a modification of gradient descent. In stochastic gradient descent, you calculate the gradient using just a random small part of the observations instead of all of them. In some cases, this approach can reduce computation time.</p>
<p><strong>Online stochastic gradient descent</strong> is a variant of stochastic gradient descent in which you estimate the gradient of the cost function for each observation and update the decision variables accordingly. This can help you find the global minimum, especially if the objective function is convex.</p>
<p><strong>Batch stochastic gradient descent</strong> is somewhere between ordinary gradient descent and the online method. The gradients are calculated and the decision variables are updated iteratively with subsets of all observations, called <strong>minibatches</strong>. This variant is very popular for training neural networks.</p>
<p>You can imagine the online algorithm as a special kind of batch algorithm in which each minibatch has only one observation. Classical gradient descent is another special case in which there&rsquo;s only one batch containing all observations.</p>
<section class="section3"><h3 id="minibatches-in-stochastic-gradient-descent">Minibatches in Stochastic Gradient Descent<a class="headerlink" href="#minibatches-in-stochastic-gradient-descent" title="Permanent link"></a></h3>
<p>As in the case of the ordinary gradient descent, stochastic gradient descent starts with an initial vector of decision variables and updates it through several iterations. The difference between the two is in what happens inside the iterations:</p>
<ul>
<li>Stochastic gradient descent randomly divides the set of observations into minibatches.</li>
<li>For each minibatch, the gradient is computed and the vector is moved.</li>
<li>Once all minibatches are used, you say that the iteration, or <strong>epoch</strong>, is finished and start the next one.</li>
</ul>
<p>This algorithm randomly selects observations for minibatches, so you need to simulate this random (or pseudorandom) behavior. You can do that with <a href="https://realpython.com/python-random/">random number generation</a>. Python has the built-in <a href="https://docs.python.org/3/library/random.html"><code>random</code></a> module, and NumPy has its own <a href="https://realpython.com/numpy-random-number-generator/">random generator</a>. The latter is more convenient when you work with arrays.</p>
<p>You&rsquo;ll create a new function called <code>sgd()</code> that is very similar to <code>gradient_descent()</code> but uses randomly selected minibatches to move along the search space:</p>
<div class="codeblock mb-3 w-100" aria-label="Code block">
  <div class="codeblock__header d-flex justify-content-between codeblock--blue">
    <span class="mr-2 noselect" aria-label="Language">Python</span>
    
    <div class="noselect">
      
    </div>
  </div>
  <div style="position: relative;">
    <div class="highlight python highlight--with-header"><pre><span></span><code><span class="linenos"> 1</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="k">def</span> <span class="nf">sgd</span><span class="p">(</span>
<span class="hll"><span class="linenos"> 4</span>    <span class="n">gradient</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">learn_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
</span><span class="hll"><span class="linenos"> 5</span>    <span class="n">tolerance</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float64&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span>
</span><span class="linenos"> 6</span><span class="p">):</span>
<span class="linenos"> 7</span>    <span class="c1"># Checking if the gradient is callable</span>
<span class="linenos"> 8</span>    <span class="k">if</span> <span class="ow">not</span> <span class="nb">callable</span><span class="p">(</span><span class="n">gradient</span><span class="p">):</span>
<span class="linenos"> 9</span>        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;&#39;gradient&#39; must be callable&quot;</span><span class="p">)</span>
<span class="linenos">10</span>
<span class="linenos">11</span>    <span class="c1"># Setting up the data type for NumPy arrays</span>
<span class="linenos">12</span>    <span class="n">dtype_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
<span class="linenos">13</span>
<span class="linenos">14</span>    <span class="c1"># Converting x and y to NumPy arrays</span>
<span class="linenos">15</span>    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">)</span>
<span class="hll"><span class="linenos">16</span>    <span class="n">n_obs</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="linenos">17</span>    <span class="k">if</span> <span class="n">n_obs</span> <span class="o">!=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
<span class="linenos">18</span>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;x&#39; and &#39;y&#39; lengths do not match&quot;</span><span class="p">)</span>
<span class="hll"><span class="linenos">19</span>    <span class="n">xy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_obs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_obs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
</span><span class="linenos">20</span>
<span class="linenos">21</span>    <span class="c1"># Initializing the random number generator</span>
<span class="hll"><span class="linenos">22</span>    <span class="n">seed</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">random_state</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">int</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
</span><span class="hll"><span class="linenos">23</span>    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
</span><span class="linenos">24</span>
<span class="linenos">25</span>    <span class="c1"># Initializing the values of the variables</span>
<span class="linenos">26</span>    <span class="n">vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">)</span>
<span class="linenos">27</span>
<span class="linenos">28</span>    <span class="c1"># Setting up and checking the learning rate</span>
<span class="linenos">29</span>    <span class="n">learn_rate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">learn_rate</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">)</span>
<span class="linenos">30</span>    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">learn_rate</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
<span class="linenos">31</span>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;learn_rate&#39; must be greater than zero&quot;</span><span class="p">)</span>
<span class="linenos">32</span>
<span class="linenos">33</span>    <span class="c1"># Setting up and checking the size of minibatches</span>
<span class="hll"><span class="linenos">34</span>    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
</span><span class="hll"><span class="linenos">35</span>    <span class="k">if</span> <span class="ow">not</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">batch_size</span> <span class="o">&lt;=</span> <span class="n">n_obs</span><span class="p">:</span>
</span><span class="hll"><span class="linenos">36</span>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span class="hll"><span class="linenos">37</span>            <span class="s2">&quot;&#39;batch_size&#39; must be greater than zero and less than &quot;</span>
</span><span class="hll"><span class="linenos">38</span>            <span class="s2">&quot;or equal to the number of observations&quot;</span>
</span><span class="hll"><span class="linenos">39</span>        <span class="p">)</span>
</span><span class="linenos">40</span>
<span class="linenos">41</span>    <span class="c1"># Setting up and checking the maximal number of iterations</span>
<span class="linenos">42</span>    <span class="n">n_iter</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_iter</span><span class="p">)</span>
<span class="linenos">43</span>    <span class="k">if</span> <span class="n">n_iter</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
<span class="linenos">44</span>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;n_iter&#39; must be greater than zero&quot;</span><span class="p">)</span>
<span class="linenos">45</span>
<span class="linenos">46</span>    <span class="c1"># Setting up and checking the tolerance</span>
<span class="linenos">47</span>    <span class="n">tolerance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">tolerance</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">)</span>
<span class="linenos">48</span>    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">tolerance</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
<span class="linenos">49</span>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;tolerance&#39; must be greater than zero&quot;</span><span class="p">)</span>
<span class="linenos">50</span>
<span class="linenos">51</span>    <span class="c1"># Performing the gradient descent loop</span>
<span class="linenos">52</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
<span class="linenos">53</span>        <span class="c1"># Shuffle x and y</span>
<span class="hll"><span class="linenos">54</span>        <span class="n">rng</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">xy</span><span class="p">)</span>
</span><span class="linenos">55</span>
<span class="linenos">56</span>        <span class="c1"># Performing minibatch moves</span>
<span class="hll"><span class="linenos">57</span>        <span class="k">for</span> <span class="n">start</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_obs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
</span><span class="hll"><span class="linenos">58</span>            <span class="n">stop</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">batch_size</span>
</span><span class="hll"><span class="linenos">59</span>            <span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">xy</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">stop</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">xy</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">stop</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
</span><span class="linenos">60</span>
<span class="linenos">61</span>            <span class="c1"># Recalculating the difference</span>
<span class="hll"><span class="linenos">62</span>            <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">gradient</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">,</span> <span class="n">vector</span><span class="p">),</span> <span class="n">dtype_</span><span class="p">)</span>
</span><span class="hll"><span class="linenos">63</span>            <span class="n">diff</span> <span class="o">=</span> <span class="o">-</span><span class="n">learn_rate</span> <span class="o">*</span> <span class="n">grad</span>
</span><span class="linenos">64</span>
<span class="linenos">65</span>            <span class="c1"># Checking if the absolute difference is small enough</span>
<span class="linenos">66</span>            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">tolerance</span><span class="p">):</span>
<span class="linenos">67</span>                <span class="k">break</span>
<span class="linenos">68</span>
<span class="linenos">69</span>            <span class="c1"># Updating the values of the variables</span>
<span class="linenos">70</span>            <span class="n">vector</span> <span class="o">+=</span> <span class="n">diff</span>
<span class="linenos">71</span>
<span class="linenos">72</span>    <span class="k">return</span> <span class="n">vector</span> <span class="k">if</span> <span class="n">vector</span><span class="o">.</span><span class="n">shape</span> <span class="k">else</span> <span class="n">vector</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</code></pre></div>
    
    <button class="codeblock__copy btn btn-outline-secondary border m-1 px-1 d-hover-only" title="Copy to clipboard"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#@copy"/></svg></span></button>
    <template class="codeblock__copied-template">
      <span class="small"><span class="icon baseline mr-1 text-success"><svg><use href="/static/icons.0c2040efe47b.svg#@check"/></svg></span>Copied!</span>
    </template>
    
  </div>
</div>
<p>You have a new parameter here. With <code>batch_size</code>, you specify the number of observations in each minibatch. This is an essential parameter for stochastic gradient descent that can significantly affect performance. Lines 34 to 39 ensure that <code>batch_size</code> is a positive integer no larger than the total number of observations.</p>
<p>Another new parameter is <code>random_state</code>. It defines the seed of the random number generator on line 22. The seed is used on line 23 as an argument to <a href="https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.default_rng"><code>default_rng()</code></a>, which creates an instance of <a href="https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.Generator"><code>Generator</code></a>.</p>
<p>If you pass the argument <a href="https://realpython.com/null-in-python/"><code>None</code></a> for <code>random_state</code>, then the random number generator will return different numbers each time it&rsquo;s instantiated. If you want each instance of the generator to behave exactly the same way, then you need to specify <code>seed</code>. The easiest way is to provide an arbitrary integer.</p>
<p>Line 16 deduces the number of observations with <code>x.shape[0]</code>. If <code>x</code> is a one-dimensional array, then this is its size. If <code>x</code> has two dimensions, then <code>.shape[0]</code> is the number of rows.</p>
<p>On line 19, you use <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.reshape.html#numpy.ndarray.reshape"><code>.reshape()</code></a> to make sure that both <code>x</code> and <code>y</code> become two-dimensional arrays with <code>n_obs</code> rows and that <code>y</code> has exactly one column. <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.reshape.html#numpy.ndarray.reshape"><code>numpy.c_[]</code></a> conveniently concatenates the columns of <code>x</code> and <code>y</code> into a single array, <code>xy</code>. This is one way to make data suitable for random selection.</p>
<p>Finally, on lines 52 to 70, you implement the <a href="https://realpython.com/python-for-loop/"><code>for</code> loop</a> for the stochastic gradient descent. It differs from <code>gradient_descent()</code>. On line 54, you use the random number generator and its method <a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.shuffle.html#numpy.random.Generator.shuffle"><code>.shuffle()</code></a> to shuffle the observations. This is one of the ways to choose minibatches randomly.</p>
<p>The inner <code>for</code> loop is repeated for each minibatch. The main difference from the ordinary gradient descent is that, on line 62, the gradient is calculated for the observations from a minibatch (<code>x_batch</code> and <code>y_batch</code>) instead of for all observations (<code>x</code> and <code>y</code>).</p>
<p>On line 59, <code>x_batch</code> becomes a part of <code>xy</code> that contains the rows of the current minibatch (from <code>start</code> to <code>stop</code>) and the columns that correspond to <code>x</code>. <code>y_batch</code> holds the same rows from <code>xy</code> but only the last column (the outputs). For more information about how indices work in NumPy, see the <a href="https://numpy.org/doc/stable/reference/arrays.indexing.html">official documentation on indexing</a>.</p>
<p>Now you can test your implementation of stochastic gradient descent:</p>
<div class="codeblock mb-3 w-100" aria-label="Code block">
  <div class="codeblock__header d-flex justify-content-between codeblock--blue">
    <span class="mr-2 noselect" aria-label="Language">Python</span>
    
    <div class="noselect">
      
        <span class="codeblock__output-toggle" title="Toggle prompts and output"><span class="icon baseline js-codeblock-output-on codeblock__header--icon-lower"><svg><use href="/static/icons.0c2040efe47b.svg#regular--rectangle-terminal"/></svg></span></span>
      
    </div>
  </div>
  <div style="position: relative;">
    <div class="highlight python repl highlight--with-header"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">sgd</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">ssr_gradient</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">learn_rate</span><span class="o">=</span><span class="mf">0.0008</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">batch_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span>
<span class="gp">... </span><span class="p">)</span>
<span class="go">array([5.63093736, 0.53982921])</span>
</code></pre></div>
    
    <button class="codeblock__copy btn btn-outline-secondary border m-1 px-1 d-hover-only" title="Copy to clipboard"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#@copy"/></svg></span></button>
    <template class="codeblock__copied-template">
      <span class="small"><span class="icon baseline mr-1 text-success"><svg><use href="/static/icons.0c2040efe47b.svg#@check"/></svg></span>Copied!</span>
    </template>
    
  </div>
</div>
<p>The result is almost the same as you got with <code>gradient_descent()</code>. If you omit <code>random_state</code> or use <code>None</code>, then you&rsquo;ll get somewhat different results each time you run <code>sgd()</code> because the random number generator will shuffle <code>xy</code> differently.</p>
<div><div class="rounded border border-light" style="display:block;position:relative;"> <div style="display:block;width:100%;padding-top:12.5%;"></div> <div class="rpad rounded border" data-unit="8x1" style="position:absolute;left:0;top:0;right:0;bottom:0;overflow:hidden;"> </div></div><a class="small text-muted" href="/account/join/" rel="nofollow"> <span class="icon baseline mr-1"><svg><use href="/static/icons.0c2040efe47b.svg#solid--circle-info"></use></svg></span>Remove ads</a></div></section><section class="section3"><h3 id="momentum-in-stochastic-gradient-descent">Momentum in Stochastic Gradient Descent<a class="headerlink" href="#momentum-in-stochastic-gradient-descent" title="Permanent link"></a></h3>
<p>As you&rsquo;ve already seen, the learning rate can have a significant impact on the result of gradient descent. You can use several different strategies for adapting the learning rate during the algorithm execution. You can also apply <strong>momentum</strong> to your algorithm.</p>
<p>You can use momentum to correct the effect of the learning rate. The idea is to remember the previous update of the vector and apply it when calculating the next one. You don&rsquo;t move the vector exactly in the direction of the negative gradient, but you also tend to keep the direction and magnitude from the previous move.</p>
<p>The parameter called the <strong>decay rate</strong> or <strong>decay factor</strong> defines how strong the contribution of the previous update is. To include the momentum and the decay rate, you can modify <code>sgd()</code> by adding the parameter <code>decay_rate</code> and use it to calculate the direction and magnitude of the vector update (<code>diff</code>):</p>
<div class="codeblock mb-3 w-100" aria-label="Code block">
  <div class="codeblock__header d-flex justify-content-between codeblock--blue">
    <span class="mr-2 noselect" aria-label="Language">Python</span>
    
    <div class="noselect">
      
    </div>
  </div>
  <div style="position: relative;">
    <div class="highlight python highlight--with-header"><pre><span></span><code><span class="linenos"> 1</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="k">def</span> <span class="nf">sgd</span><span class="p">(</span>
<span class="hll"><span class="linenos"> 4</span>    <span class="n">gradient</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">learn_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span class="linenos"> 5</span>    <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float64&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span>
<span class="linenos"> 6</span><span class="p">):</span>
<span class="linenos"> 7</span>    <span class="c1"># Checking if the gradient is callable</span>
<span class="linenos"> 8</span>    <span class="k">if</span> <span class="ow">not</span> <span class="nb">callable</span><span class="p">(</span><span class="n">gradient</span><span class="p">):</span>
<span class="linenos"> 9</span>        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;&#39;gradient&#39; must be callable&quot;</span><span class="p">)</span>
<span class="linenos">10</span>
<span class="linenos">11</span>    <span class="c1"># Setting up the data type for NumPy arrays</span>
<span class="linenos">12</span>    <span class="n">dtype_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
<span class="linenos">13</span>
<span class="linenos">14</span>    <span class="c1"># Converting x and y to NumPy arrays</span>
<span class="linenos">15</span>    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">)</span>
<span class="linenos">16</span>    <span class="n">n_obs</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="linenos">17</span>    <span class="k">if</span> <span class="n">n_obs</span> <span class="o">!=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
<span class="linenos">18</span>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;x&#39; and &#39;y&#39; lengths do not match&quot;</span><span class="p">)</span>
<span class="linenos">19</span>    <span class="n">xy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_obs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_obs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
<span class="linenos">20</span>
<span class="linenos">21</span>    <span class="c1"># Initializing the random number generator</span>
<span class="linenos">22</span>    <span class="n">seed</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">random_state</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">int</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
<span class="linenos">23</span>    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="linenos">24</span>
<span class="linenos">25</span>    <span class="c1"># Initializing the values of the variables</span>
<span class="linenos">26</span>    <span class="n">vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">)</span>
<span class="linenos">27</span>
<span class="linenos">28</span>    <span class="c1"># Setting up and checking the learning rate</span>
<span class="linenos">29</span>    <span class="n">learn_rate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">learn_rate</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">)</span>
<span class="linenos">30</span>    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">learn_rate</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
<span class="linenos">31</span>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;learn_rate&#39; must be greater than zero&quot;</span><span class="p">)</span>
<span class="linenos">32</span>
<span class="linenos">33</span>    <span class="c1"># Setting up and checking the decay rate</span>
<span class="hll"><span class="linenos">34</span>    <span class="n">decay_rate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">decay_rate</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">)</span>
</span><span class="hll"><span class="linenos">35</span>    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">decay_rate</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">decay_rate</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">):</span>
</span><span class="hll"><span class="linenos">36</span>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;decay_rate&#39; must be between zero and one&quot;</span><span class="p">)</span>
</span><span class="linenos">37</span>
<span class="linenos">38</span>    <span class="c1"># Setting up and checking the size of minibatches</span>
<span class="linenos">39</span>    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
<span class="linenos">40</span>    <span class="k">if</span> <span class="ow">not</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">batch_size</span> <span class="o">&lt;=</span> <span class="n">n_obs</span><span class="p">:</span>
<span class="linenos">41</span>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<span class="linenos">42</span>            <span class="s2">&quot;&#39;batch_size&#39; must be greater than zero and less than &quot;</span>
<span class="linenos">43</span>            <span class="s2">&quot;or equal to the number of observations&quot;</span>
<span class="linenos">44</span>        <span class="p">)</span>
<span class="linenos">45</span>
<span class="linenos">46</span>    <span class="c1"># Setting up and checking the maximal number of iterations</span>
<span class="linenos">47</span>    <span class="n">n_iter</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_iter</span><span class="p">)</span>
<span class="linenos">48</span>    <span class="k">if</span> <span class="n">n_iter</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
<span class="linenos">49</span>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;n_iter&#39; must be greater than zero&quot;</span><span class="p">)</span>
<span class="linenos">50</span>
<span class="linenos">51</span>    <span class="c1"># Setting up and checking the tolerance</span>
<span class="linenos">52</span>    <span class="n">tolerance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">tolerance</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">)</span>
<span class="linenos">53</span>    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">tolerance</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
<span class="linenos">54</span>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;tolerance&#39; must be greater than zero&quot;</span><span class="p">)</span>
<span class="linenos">55</span>
<span class="linenos">56</span>    <span class="c1"># Setting the difference to zero for the first iteration</span>
<span class="hll"><span class="linenos">57</span>    <span class="n">diff</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="linenos">58</span>
<span class="linenos">59</span>    <span class="c1"># Performing the gradient descent loop</span>
<span class="linenos">60</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
<span class="linenos">61</span>        <span class="c1"># Shuffle x and y</span>
<span class="linenos">62</span>        <span class="n">rng</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">xy</span><span class="p">)</span>
<span class="linenos">63</span>
<span class="linenos">64</span>        <span class="c1"># Performing minibatch moves</span>
<span class="linenos">65</span>        <span class="k">for</span> <span class="n">start</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_obs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
<span class="linenos">66</span>            <span class="n">stop</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">batch_size</span>
<span class="linenos">67</span>            <span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">xy</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">stop</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">xy</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">stop</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
<span class="linenos">68</span>
<span class="linenos">69</span>            <span class="c1"># Recalculating the difference</span>
<span class="linenos">70</span>            <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">gradient</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">,</span> <span class="n">vector</span><span class="p">),</span> <span class="n">dtype_</span><span class="p">)</span>
<span class="hll"><span class="linenos">71</span>            <span class="n">diff</span> <span class="o">=</span> <span class="n">decay_rate</span> <span class="o">*</span> <span class="n">diff</span> <span class="o">-</span> <span class="n">learn_rate</span> <span class="o">*</span> <span class="n">grad</span>
</span><span class="linenos">72</span>
<span class="linenos">73</span>            <span class="c1"># Checking if the absolute difference is small enough</span>
<span class="linenos">74</span>            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">tolerance</span><span class="p">):</span>
<span class="linenos">75</span>                <span class="k">break</span>
<span class="linenos">76</span>
<span class="linenos">77</span>            <span class="c1"># Updating the values of the variables</span>
<span class="linenos">78</span>            <span class="n">vector</span> <span class="o">+=</span> <span class="n">diff</span>
<span class="linenos">79</span>
<span class="linenos">80</span>    <span class="k">return</span> <span class="n">vector</span> <span class="k">if</span> <span class="n">vector</span><span class="o">.</span><span class="n">shape</span> <span class="k">else</span> <span class="n">vector</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</code></pre></div>
    
    <button class="codeblock__copy btn btn-outline-secondary border m-1 px-1 d-hover-only" title="Copy to clipboard"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#@copy"/></svg></span></button>
    <template class="codeblock__copied-template">
      <span class="small"><span class="icon baseline mr-1 text-success"><svg><use href="/static/icons.0c2040efe47b.svg#@check"/></svg></span>Copied!</span>
    </template>
    
  </div>
</div>
<p>In this implementation, you add the <code>decay_rate</code> parameter on line 4, convert it to a NumPy array of the desired type on line 34, and check if it&rsquo;s between zero and one on lines 35 and 36. On line 57, you initialize <code>diff</code> before the iterations start to ensure that it&rsquo;s available in the first iteration.</p>
<p>The most important change happens on line 71. You recalculate <code>diff</code> with the learning rate and gradient but also add the product of the decay rate and the old value of <code>diff</code>. Now <code>diff</code> has two components:</p>
<ol>
<li><strong><code>decay_rate * diff</code></strong> is the momentum, or impact of the previous move.</li>
<li><strong><code>-learn_rate * grad</code></strong> is the impact of the current gradient.</li>
</ol>
<p>The decay and learning rates serve as the weights that define the contributions of the two.</p>
</section><section class="section3"><h3 id="random-start-values">Random Start Values<a class="headerlink" href="#random-start-values" title="Permanent link"></a></h3>
<p>As opposed to ordinary gradient descent, the starting point is often not so important for stochastic gradient descent. It may also be an unnecessary difficulty for a user, especially when you have many decision variables. To get an idea, just imagine if you needed to manually initialize the values for a neural network with thousands of biases and weights!</p>
<p>In practice, you can start with some small arbitrary values. You&rsquo;ll use the random number generator to get them:</p>
<div class="codeblock mb-3 w-100" aria-label="Code block">
  <div class="codeblock__header d-flex justify-content-between codeblock--blue">
    <span class="mr-2 noselect" aria-label="Language">Python</span>
    
    <div class="noselect">
      
    </div>
  </div>
  <div style="position: relative;">
    <div class="highlight python highlight--with-header"><pre><span></span><code><span class="linenos"> 1</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="k">def</span> <span class="nf">sgd</span><span class="p">(</span>
<span class="hll"><span class="linenos"> 4</span>    <span class="n">gradient</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n_vars</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">learn_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
</span><span class="linenos"> 5</span>    <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">,</span>
<span class="linenos"> 6</span>    <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float64&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span>
<span class="linenos"> 7</span><span class="p">):</span>
<span class="linenos"> 8</span>    <span class="c1"># Checking if the gradient is callable</span>
<span class="linenos"> 9</span>    <span class="k">if</span> <span class="ow">not</span> <span class="nb">callable</span><span class="p">(</span><span class="n">gradient</span><span class="p">):</span>
<span class="linenos">10</span>        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;&#39;gradient&#39; must be callable&quot;</span><span class="p">)</span>
<span class="linenos">11</span>
<span class="linenos">12</span>    <span class="c1"># Setting up the data type for NumPy arrays</span>
<span class="linenos">13</span>    <span class="n">dtype_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
<span class="linenos">14</span>
<span class="linenos">15</span>    <span class="c1"># Converting x and y to NumPy arrays</span>
<span class="linenos">16</span>    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">)</span>
<span class="linenos">17</span>    <span class="n">n_obs</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="linenos">18</span>    <span class="k">if</span> <span class="n">n_obs</span> <span class="o">!=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
<span class="linenos">19</span>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;x&#39; and &#39;y&#39; lengths do not match&quot;</span><span class="p">)</span>
<span class="linenos">20</span>    <span class="n">xy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_obs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_obs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
<span class="linenos">21</span>
<span class="linenos">22</span>    <span class="c1"># Initializing the random number generator</span>
<span class="linenos">23</span>    <span class="n">seed</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">random_state</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">int</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
<span class="linenos">24</span>    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="linenos">25</span>
<span class="linenos">26</span>    <span class="c1"># Initializing the values of the variables</span>
<span class="hll"><span class="linenos">27</span>    <span class="n">vector</span> <span class="o">=</span> <span class="p">(</span>
</span><span class="hll"><span class="linenos">28</span>        <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">n_vars</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype_</span><span class="p">)</span>
</span><span class="hll"><span class="linenos">29</span>        <span class="k">if</span> <span class="n">start</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span>
</span><span class="hll"><span class="linenos">30</span>        <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">)</span>
</span><span class="hll"><span class="linenos">31</span>    <span class="p">)</span>
</span><span class="linenos">32</span>
<span class="linenos">33</span>    <span class="c1"># Setting up and checking the learning rate</span>
<span class="linenos">34</span>    <span class="n">learn_rate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">learn_rate</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">)</span>
<span class="linenos">35</span>    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">learn_rate</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
<span class="linenos">36</span>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;learn_rate&#39; must be greater than zero&quot;</span><span class="p">)</span>
<span class="linenos">37</span>
<span class="linenos">38</span>    <span class="c1"># Setting up and checking the decay rate</span>
<span class="linenos">39</span>    <span class="n">decay_rate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">decay_rate</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">)</span>
<span class="linenos">40</span>    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">decay_rate</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">decay_rate</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">):</span>
<span class="linenos">41</span>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;decay_rate&#39; must be between zero and one&quot;</span><span class="p">)</span>
<span class="linenos">42</span>
<span class="linenos">43</span>    <span class="c1"># Setting up and checking the size of minibatches</span>
<span class="linenos">44</span>    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
<span class="linenos">45</span>    <span class="k">if</span> <span class="ow">not</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">batch_size</span> <span class="o">&lt;=</span> <span class="n">n_obs</span><span class="p">:</span>
<span class="linenos">46</span>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<span class="linenos">47</span>            <span class="s2">&quot;&#39;batch_size&#39; must be greater than zero and less than &quot;</span>
<span class="linenos">48</span>            <span class="s2">&quot;or equal to the number of observations&quot;</span>
<span class="linenos">49</span>        <span class="p">)</span>
<span class="linenos">50</span>
<span class="linenos">51</span>    <span class="c1"># Setting up and checking the maximal number of iterations</span>
<span class="linenos">52</span>    <span class="n">n_iter</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_iter</span><span class="p">)</span>
<span class="linenos">53</span>    <span class="k">if</span> <span class="n">n_iter</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
<span class="linenos">54</span>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;n_iter&#39; must be greater than zero&quot;</span><span class="p">)</span>
<span class="linenos">55</span>
<span class="linenos">56</span>    <span class="c1"># Setting up and checking the tolerance</span>
<span class="linenos">57</span>    <span class="n">tolerance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">tolerance</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">)</span>
<span class="linenos">58</span>    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">tolerance</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
<span class="linenos">59</span>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;tolerance&#39; must be greater than zero&quot;</span><span class="p">)</span>
<span class="linenos">60</span>
<span class="linenos">61</span>    <span class="c1"># Setting the difference to zero for the first iteration</span>
<span class="linenos">62</span>    <span class="n">diff</span> <span class="o">=</span> <span class="mi">0</span>
<span class="linenos">63</span>
<span class="linenos">64</span>    <span class="c1"># Performing the gradient descent loop</span>
<span class="linenos">65</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
<span class="linenos">66</span>        <span class="c1"># Shuffle x and y</span>
<span class="linenos">67</span>        <span class="n">rng</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">xy</span><span class="p">)</span>
<span class="linenos">68</span>
<span class="linenos">69</span>        <span class="c1"># Performing minibatch moves</span>
<span class="linenos">70</span>        <span class="k">for</span> <span class="n">start</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_obs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
<span class="linenos">71</span>            <span class="n">stop</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">batch_size</span>
<span class="linenos">72</span>            <span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">xy</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">stop</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">xy</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">stop</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
<span class="linenos">73</span>
<span class="linenos">74</span>            <span class="c1"># Recalculating the difference</span>
<span class="linenos">75</span>            <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">gradient</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">,</span> <span class="n">vector</span><span class="p">),</span> <span class="n">dtype_</span><span class="p">)</span>
<span class="linenos">76</span>            <span class="n">diff</span> <span class="o">=</span> <span class="n">decay_rate</span> <span class="o">*</span> <span class="n">diff</span> <span class="o">-</span> <span class="n">learn_rate</span> <span class="o">*</span> <span class="n">grad</span>
<span class="linenos">77</span>
<span class="linenos">78</span>            <span class="c1"># Checking if the absolute difference is small enough</span>
<span class="linenos">79</span>            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">tolerance</span><span class="p">):</span>
<span class="linenos">80</span>                <span class="k">break</span>
<span class="linenos">81</span>
<span class="linenos">82</span>            <span class="c1"># Updating the values of the variables</span>
<span class="linenos">83</span>            <span class="n">vector</span> <span class="o">+=</span> <span class="n">diff</span>
<span class="linenos">84</span>
<span class="linenos">85</span>    <span class="k">return</span> <span class="n">vector</span> <span class="k">if</span> <span class="n">vector</span><span class="o">.</span><span class="n">shape</span> <span class="k">else</span> <span class="n">vector</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</code></pre></div>
    
    <button class="codeblock__copy btn btn-outline-secondary border m-1 px-1 d-hover-only" title="Copy to clipboard"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#@copy"/></svg></span></button>
    <template class="codeblock__copied-template">
      <span class="small"><span class="icon baseline mr-1 text-success"><svg><use href="/static/icons.0c2040efe47b.svg#@check"/></svg></span>Copied!</span>
    </template>
    
  </div>
</div>
<p>You now have the new parameter <code>n_vars</code> that defines the number of decision variables in your problem. The parameter <code>start</code> is optional and has the default value <code>None</code>. Lines 27 to 31 initialize the starting values of the decision variables:</p>
<ul>
<li>If you provide a <code>start</code> value other than <code>None</code>, then it&rsquo;s used for the starting values.</li>
<li>If <code>start</code> is <code>None</code>, then your random number generator creates the starting values using the <a href="https://en.wikipedia.org/wiki/Normal_distribution#Standard_normal_distribution">standard normal distribution</a> and the NumPy method <a href="https://realpython.com/numpy-random-normal/"><code>.normal()</code></a>.</li>
</ul>
<p>Now give <code>sgd()</code> a try:</p>
<div class="codeblock mb-3 w-100" aria-label="Code block">
  <div class="codeblock__header d-flex justify-content-between codeblock--blue">
    <span class="mr-2 noselect" aria-label="Language">Python</span>
    
    <div class="noselect">
      
        <span class="codeblock__output-toggle" title="Toggle prompts and output"><span class="icon baseline js-codeblock-output-on codeblock__header--icon-lower"><svg><use href="/static/icons.0c2040efe47b.svg#regular--rectangle-terminal"/></svg></span></span>
      
    </div>
  </div>
  <div style="position: relative;">
    <div class="highlight python repl highlight--with-header"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">sgd</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">ssr_gradient</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n_vars</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">learn_rate</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span>
<span class="gp">... </span><span class="p">)</span>
<span class="go">array([5.63014443, 0.53901017])</span>
</code></pre></div>
    
    <button class="codeblock__copy btn btn-outline-secondary border m-1 px-1 d-hover-only" title="Copy to clipboard"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#@copy"/></svg></span></button>
    <template class="codeblock__copied-template">
      <span class="small"><span class="icon baseline mr-1 text-success"><svg><use href="/static/icons.0c2040efe47b.svg#@check"/></svg></span>Copied!</span>
    </template>
    
  </div>
</div>
<p>You get similar results again.</p>
<p>You&rsquo;ve learned how to write the functions that implement gradient descent and stochastic gradient descent. The code above can be made more robust and polished. You can also find different implementations of these methods in well-known machine learning libraries.</p>
</section></section><section class="section2"><h2 id="gradient-descent-in-keras-and-tensorflow">Gradient Descent in Keras and TensorFlow<a class="headerlink" href="#gradient-descent-in-keras-and-tensorflow" title="Permanent link"></a></h2>
<p>Stochastic gradient descent is widely used to train neural networks. The libraries for neural networks often have different variants of optimization algorithms based on stochastic gradient descent, such as:</p>
<ul>
<li>Adam</li>
<li>Adagrad</li>
<li>Adadelta</li>
<li>RMSProp</li>
</ul>
<p>These optimization libraries are usually called internally when neural network software is trained. However, you can use them independently as well:</p>
<div class="codeblock mb-3 w-100" aria-label="Code block">
  <div class="codeblock__header d-flex justify-content-between codeblock--blue">
    <span class="mr-2 noselect" aria-label="Language">Python</span>
    
    <div class="noselect">
      
        <span class="codeblock__output-toggle" title="Toggle prompts and output"><span class="icon baseline js-codeblock-output-on codeblock__header--icon-lower"><svg><use href="/static/icons.0c2040efe47b.svg#regular--rectangle-terminal"/></svg></span></span>
      
    </div>
  </div>
  <div style="position: relative;">
    <div class="highlight python repl highlight--with-header"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Create needed objects</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sgd</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">var</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">2.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cost</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">var</span> <span class="o">**</span> <span class="mi">2</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Perform optimization</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">sgd</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="p">[</span><span class="n">var</span><span class="p">])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Extract results</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">var</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="go">-0.007128528</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cost</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="go">2.0000508</span>
</code></pre></div>
    
    <button class="codeblock__copy btn btn-outline-secondary border m-1 px-1 d-hover-only" title="Copy to clipboard"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#@copy"/></svg></span></button>
    <template class="codeblock__copied-template">
      <span class="small"><span class="icon baseline mr-1 text-success"><svg><use href="/static/icons.0c2040efe47b.svg#@check"/></svg></span>Copied!</span>
    </template>
    
  </div>
</div>
<p>In this example, you first import <code>tensorflow</code> and then create the object needed for optimization:</p>
<ul>
<li><strong><code>sgd</code></strong> is an instance of the stochastic gradient descent optimizer with a learning rate of <code>0.1</code> and a momentum of <code>0.9</code>.</li>
<li><strong><code>var</code></strong> is an instance of the decision variable with an initial value of <code>2.5</code>.</li>
<li><strong><code>cost</code></strong> is the cost function, which is a square function in this case.</li>
</ul>
<p>The main part of the code is a <code>for</code> loop that iteratively calls <code>.minimize()</code> and modifies <code>var</code> and <code>cost</code>. Once the loop is exhausted, you can get the values of the decision variable and the cost function with <code>.numpy()</code>.</p>
<p>You can find more information on these algorithms in the <a href="https://keras.io/api/optimizers/">Keras</a> and <a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/">TensorFlow</a> documentation. The article <a href="https://ruder.io/optimizing-gradient-descent/">An overview of gradient descent optimization algorithms</a> offers a comprehensive list with explanations of gradient descent variants.</p>
<div><div class="rounded border border-light" style="display:block;position:relative;"> <div style="display:block;width:100%;padding-top:12.5%;"></div> <div class="rpad rounded border" data-unit="8x1" style="position:absolute;left:0;top:0;right:0;bottom:0;overflow:hidden;"> </div></div><a class="small text-muted" href="/account/join/" rel="nofollow"> <span class="icon baseline mr-1"><svg><use href="/static/icons.0c2040efe47b.svg#solid--circle-info"></use></svg></span>Remove ads</a></div></section><section class="section2"><h2 id="conclusion">Conclusion<a class="headerlink" href="#conclusion" title="Permanent link"></a></h2>
<p>You now know what <strong>gradient descent</strong> and <strong>stochastic gradient descent</strong> algorithms are and how they work. They&rsquo;re widely used in the applications of artificial neural networks and are implemented in popular libraries like Keras and TensorFlow.</p>
<p><strong>In this tutorial, you&rsquo;ve learned:</strong></p>
<ul>
<li>How to <strong>write your own functions</strong> for gradient descent and stochastic gradient descent</li>
<li>How to <strong>apply your functions</strong> to solve optimization problems</li>
<li>What the <strong>key features and concepts</strong> of gradient descent are, like learning rate or momentum, as well as its limitations </li>
</ul>
<p>You&rsquo;ve used gradient descent and stochastic gradient descent to find the minima of several functions and to fit the regression line in a linear regression problem. You&rsquo;ve also seen how to apply the class <code>SGD</code> from TensorFlow that&rsquo;s used to train neural networks.</p>
<p>If you have questions or comments, then please put them in the comment section below.</p>
</section>

    
      
      <div class="text-center my-3">
        
<div class="jsCompletionStatusWidget btn-group mb-0">
  <button title="Click to mark as completed" class="jsBtnCompletion btn btn-secondary border-right " style="border-top-right-radius: 0; border-bottom-right-radius: 0;" disabled>Mark as Completed</button>
  <button title="Add bookmark" class="jsBtnBookmark btn btn-secondary border-left" disabled><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#light--bookmark"/></svg></span></button>
</div>

        <span class="ml-2">
<div class="btn-group mb-0">
  <a class="btn btn-secondary border-right" style="border-top-right-radius: 0; border-bottom-right-radius: 0;" title="Liked it" role="button" aria-label="Thumbs up (liked it)" href="/feedback/survey/article/gradient-descent-algorithm-python/liked/?from=article-footer" target="_blank"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#light--thumbs-up"/></svg></span></a>
  <a class="btn btn-secondary border-left" role="button" aria-label="Thumbs down (disliked it)" title="Disliked it" href="/feedback/survey/article/gradient-descent-algorithm-python/disliked/?from=article-footer" target="_blank"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#light--thumbs-down"/></svg></span></a>
</div>
</span>
      </div>
    

    
  </div>

  
    

<div class="card mt-4 mb-4 bg-secondary">
  <p class="card-header h3 text-center bg-light">🐍 Python Tricks 💌</p>
  <div class="card-body">
    <div class="container">
      <div class="row">
        <div class="col-xs-12 col-sm-7">
          <p>Get a short &amp; sweet <strong>Python Trick</strong> delivered to your inbox every couple of days. No spam ever. Unsubscribe any time. Curated by the Real Python team.</p>
        </div>
        <div class="col-xs-12 col-sm-5">
          <img loading="lazy" class="img-fluid rounded mb-3" src="/static/pytrick-dict-merge.4201a0125a5e.png" width="738" height="490" alt="Python Tricks Dictionary Merge">
        </div>
      </div>
      <div class="row mb-3">
        <form class="col-12" action="/optins/process/" method="post">
          <input type="hidden" name="csrfmiddlewaretoken" value="WMo7jZKy0VzLokBMAryCrBUj4p6InOrGDcUwE7VRo9NWcEXEivCM7yz9u9xak2EP">
          <input type="hidden" name="slug" value="static-python-tricks-footer">
          <div class="form-group">
            <input name="email" type="email" class="form-control form-control-lg" placeholder="Email Address" required>
          </div>
          <button name="submit" type="submit" class="btn btn-primary btn-lg btn-block">Send Me Python Tricks »</button>
        </form>
      </div>
    </div>
  </div>
</div>


  

  



<div class="card mt-3" id="author">
  <p class="card-header h3">About <strong>Mirko Stojiljković</strong></p>
  <div class="card-body">
    <div class="container p-0">
      <div class="row">
        <div class="col-12 col-md-3 align-self-center">
          <a href="/team/mstojiljkovic/"><img loading="lazy" src="https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/ms.fdcd0bdc2f4a.png&amp;w=240&amp;h=240&amp;mode=crop&amp;sig=3952dbe00532973b3680e177512373bbdcdbbe75" srcset="https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/ms.fdcd0bdc2f4a.png&amp;w=60&amp;h=60&amp;mode=crop&amp;sig=b538cfe117b64a6189f297e361f4aea7b3bf4298 60w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/ms.fdcd0bdc2f4a.png&amp;w=80&amp;h=80&amp;mode=crop&amp;sig=5e9ab2176d8ef40943532b2380fcdce8941ec625 80w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/ms.fdcd0bdc2f4a.png&amp;w=120&amp;h=120&amp;mode=crop&amp;sig=441f902e060a34910f8c426ebb02b5560078d98f 120w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/ms.fdcd0bdc2f4a.png&amp;w=240&amp;h=240&amp;mode=crop&amp;sig=3952dbe00532973b3680e177512373bbdcdbbe75 240w" sizes="(min-width: 580px) 154px, calc(33.08vw - 24px)" width="240" height="240" style="background: #7e7678;" class="d-block d-md-none rounded-circle img-fluid w-33 mb-0 mx-auto" alt="Mirko Stojiljković"></a>
          <a href="/team/mstojiljkovic/"><img loading="lazy" src="https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/ms.fdcd0bdc2f4a.png&amp;w=240&amp;h=240&amp;mode=crop&amp;sig=3952dbe00532973b3680e177512373bbdcdbbe75" srcset="https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/ms.fdcd0bdc2f4a.png&amp;w=60&amp;h=60&amp;mode=crop&amp;sig=b538cfe117b64a6189f297e361f4aea7b3bf4298 60w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/ms.fdcd0bdc2f4a.png&amp;w=80&amp;h=80&amp;mode=crop&amp;sig=5e9ab2176d8ef40943532b2380fcdce8941ec625 80w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/ms.fdcd0bdc2f4a.png&amp;w=120&amp;h=120&amp;mode=crop&amp;sig=441f902e060a34910f8c426ebb02b5560078d98f 120w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/ms.fdcd0bdc2f4a.png&amp;w=240&amp;h=240&amp;mode=crop&amp;sig=3952dbe00532973b3680e177512373bbdcdbbe75 240w" sizes="(min-width: 1200px) 140px, calc(-1.5vw + 137px)" width="240" height="240" style="background: #7e7678;" class="d-none d-md-block rounded-circle img-fluid w-100 mb-0" alt="Mirko Stojiljković"></a>
        </div>
        <div class="col mt-3">
          <p>Mirko has a Ph.D. in Mechanical Engineering and works as a university professor. He is a Pythonista who applies hybrid optimization and machine learning methods to support decision making in the energy sector.</p>
          <a href="/team/mstojiljkovic/" class="card-link">» More about Mirko</a>
        </div>
      </div>
    </div>
  </div>
  
  <hr class="my-0">
  <div class="card-body pb-0">
    <div class="container">
      <div class="row">
        <p><em>Each tutorial at Real Python is created by a team of developers so that it meets our high quality standards. The team members who worked on this tutorial are:</em></p>
      </div>

      
        
          <div class="row align-items-center w-100 mx-auto">
        

        <div class="col-4 col-sm-2 align-self-center">
          
            <a href="/team/asantos/"><img loading="lazy" src="https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/PP.9b8b026f75b8.jpg&amp;w=959&amp;h=959&amp;mode=crop&amp;sig=70bedc2eab90a227eb9a657c415689c3eb1eca4f" srcset="https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/PP.9b8b026f75b8.jpg&amp;w=239&amp;h=239&amp;mode=crop&amp;sig=11667a6dd5c29e4c9363f18be59360551af5eddc 239w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/PP.9b8b026f75b8.jpg&amp;w=319&amp;h=319&amp;mode=crop&amp;sig=3b626fde72a314418766c7d4a14e58ec6f67da7b 319w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/PP.9b8b026f75b8.jpg&amp;w=479&amp;h=479&amp;mode=crop&amp;sig=1541e1ec541357813def826d8507c0565164b701 479w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/PP.9b8b026f75b8.jpg&amp;w=959&amp;h=959&amp;mode=crop&amp;sig=70bedc2eab90a227eb9a657c415689c3eb1eca4f 959w" sizes="(min-width: 1200px) 73px, (min-width: 780px) calc(-0.75vw + 69px), (min-width: 580px) 43px, calc(33.46vw - 64px)" width="959" height="959" style="background: #dadad8;" class="rounded-circle img-fluid w-100" alt="Aldren Santos"></a>
          
        </div>
        <div class="col pl-0 d-none d-sm-block">
          <a href="/team/asantos/" class="card-link small"><p>Aldren</p></a>
        </div>

        

        
      
        

        <div class="col-4 col-sm-2 align-self-center">
          
            <a href="/team/bzaczynski/"><img loading="lazy" src="https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/coders_lab_2109368.259b1599fbee.jpg&amp;w=1694&amp;h=1694&amp;mode=crop&amp;sig=de07b3f1b4a9c51d44ecaca8faf56c6211353f86" srcset="https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/coders_lab_2109368.259b1599fbee.jpg&amp;w=423&amp;h=423&amp;mode=crop&amp;sig=f95161b4b06c56dcce1ba91727b834fb49fb116f 423w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/coders_lab_2109368.259b1599fbee.jpg&amp;w=564&amp;h=564&amp;mode=crop&amp;sig=379db593d0e7d2369708be1410ed7a59bf23492c 564w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/coders_lab_2109368.259b1599fbee.jpg&amp;w=847&amp;h=847&amp;mode=crop&amp;sig=e359a3e6f40e1d56d25c5f02ee2e6e19f93867db 847w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/coders_lab_2109368.259b1599fbee.jpg&amp;w=1694&amp;h=1694&amp;mode=crop&amp;sig=de07b3f1b4a9c51d44ecaca8faf56c6211353f86 1694w" sizes="(min-width: 1200px) 73px, (min-width: 780px) calc(-0.75vw + 69px), (min-width: 580px) 43px, calc(33.46vw - 64px)" width="1694" height="1694" style="background: #dadada;" class="rounded-circle img-fluid w-100" alt="Bartosz Zaczyński"></a>
          
        </div>
        <div class="col pl-0 d-none d-sm-block">
          <a href="/team/bzaczynski/" class="card-link small"><p>Bartosz</p></a>
        </div>

        

        
      
        

        <div class="col-4 col-sm-2 align-self-center">
          
            <a href="/team/gahjelle/"><img loading="lazy" src="https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/gahjelle.470149ee709e.jpg&amp;w=800&amp;h=800&amp;mode=crop&amp;sig=e9b761c6cf1359953014dba05554f5424eb116e1" srcset="https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/gahjelle.470149ee709e.jpg&amp;w=200&amp;h=200&amp;mode=crop&amp;sig=c6390201e73d3e09429d73da5bb29c17ab10403a 200w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/gahjelle.470149ee709e.jpg&amp;w=266&amp;h=266&amp;mode=crop&amp;sig=7df7c39b123c3f9d8a4597311d09c3bd947f5fac 266w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/gahjelle.470149ee709e.jpg&amp;w=400&amp;h=400&amp;mode=crop&amp;sig=fcea459ee24a7b320573cadee324cf75509dc1d6 400w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/gahjelle.470149ee709e.jpg&amp;w=800&amp;h=800&amp;mode=crop&amp;sig=e9b761c6cf1359953014dba05554f5424eb116e1 800w" sizes="(min-width: 1200px) 73px, (min-width: 780px) calc(-0.75vw + 69px), (min-width: 580px) 43px, calc(33.46vw - 64px)" width="800" height="800" style="background: #080a09;" class="rounded-circle img-fluid w-100" alt="Geir Arne Hjelle"></a>
          
        </div>
        <div class="col pl-0 d-none d-sm-block">
          <a href="/team/gahjelle/" class="card-link small"><p>Geir Arne</p></a>
        </div>

        

        
          </div>
        
      
        
          <div class="row align-items-center w-100 mx-auto">
        

        <div class="col-4 col-sm-2 align-self-center">
          
            <a href="/team/jjablonski/"><img loading="lazy" src="https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/jjablonksi-avatar.e37c4f83308e.jpg&amp;w=800&amp;h=800&amp;mode=crop&amp;sig=c363b704eeccb35f2247db13baff3d4383459858" srcset="https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/jjablonksi-avatar.e37c4f83308e.jpg&amp;w=200&amp;h=200&amp;mode=crop&amp;sig=706b16de3cb88a8f353f4a98d7c7bc7234229bd0 200w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/jjablonksi-avatar.e37c4f83308e.jpg&amp;w=266&amp;h=266&amp;mode=crop&amp;sig=3003d96c12fd06ac995e52fcac8d9094a4a557b5 266w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/jjablonksi-avatar.e37c4f83308e.jpg&amp;w=400&amp;h=400&amp;mode=crop&amp;sig=6d7aa672ca3f1ac5f7cd62ed1641b60f98d04d8b 400w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/jjablonksi-avatar.e37c4f83308e.jpg&amp;w=800&amp;h=800&amp;mode=crop&amp;sig=c363b704eeccb35f2247db13baff3d4383459858 800w" sizes="(min-width: 1200px) 73px, (min-width: 780px) calc(-0.75vw + 69px), (min-width: 580px) 43px, calc(33.46vw - 64px)" width="800" height="800" style="background: #b8b9bb;" class="rounded-circle img-fluid w-100" alt="Joanna Jablonski"></a>
          
        </div>
        <div class="col pl-0 d-none d-sm-block">
          <a href="/team/jjablonski/" class="card-link small"><p>Joanna</p></a>
        </div>

        

        
      
        

        <div class="col-4 col-sm-2 align-self-center">
          
            <a href="/team/jschmitt/"><img loading="lazy" src="https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/profile-small_js.2f4d0d8da1ca.jpg&amp;w=400&amp;h=400&amp;mode=crop&amp;sig=d10d9fc35ba4a6608969e71b4c24c1e61176ee2d" srcset="https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/profile-small_js.2f4d0d8da1ca.jpg&amp;w=100&amp;h=100&amp;mode=crop&amp;sig=ef40d1115d3b4c306b16314b5555d5dc55361da9 100w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/profile-small_js.2f4d0d8da1ca.jpg&amp;w=133&amp;h=133&amp;mode=crop&amp;sig=82bd90e51eb5f48f8fe84ab4c528a9fb587e3aae 133w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/profile-small_js.2f4d0d8da1ca.jpg&amp;w=200&amp;h=200&amp;mode=crop&amp;sig=e1df2f238effe79f5750fc75258642036de498c3 200w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/profile-small_js.2f4d0d8da1ca.jpg&amp;w=400&amp;h=400&amp;mode=crop&amp;sig=d10d9fc35ba4a6608969e71b4c24c1e61176ee2d 400w" sizes="(min-width: 1200px) 73px, (min-width: 780px) calc(-0.75vw + 69px), (min-width: 580px) 43px, calc(33.46vw - 64px)" width="400" height="400" style="background: #c0c0c0;" class="rounded-circle img-fluid w-100" alt="Jacob Schmitt"></a>
          
        </div>
        <div class="col pl-0 d-none d-sm-block">
          <a href="/team/jschmitt/" class="card-link small"><p>Jacob</p></a>
        </div>

        
          
          
            <div class="col-4 col-sm-2 align-self-center"></div>
            <div class="col pl-0 d-none d-sm-block"></div>
          
        

        
          </div>
        
      
    </div>
  </div>
  

</div>





  
    

<div class="bg-light rounded py-4 my-4 shadow shadow-sm mx-n2">
  
  <div class="col-12 text-center d-block d-md-none">
    <p class="h2 mb-3">Master <u><span class="marker-highlight">Real-World Python Skills</mark></u> With Unlimited Access to Real&nbsp;Python</p>
    <p class="mb-1"><img loading="lazy" class="w-75" src="/static/videos/lesson-locked.f5105cfd26db.svg" width="510" height="260"></p>
    <p class="mx-auto w-75 mb-3 small"><strong>Join us and get access to thousands of tutorials, hands-on video courses, and a community of expert&nbsp;Pythonistas:</strong></p>
    <p class="mb-0"><a href="/account/join/?utm_source=rp_article_footer&utm_content=gradient-descent-algorithm-python" class="btn btn-primary btn-sm px-4 mb-0">Level Up Your Python Skills »</a>
  </div>

  
  <div class="col-12 text-center d-none d-md-block">
    <p class="h2 mb-2">Master <u><span class="marker-highlight">Real-World Python Skills</span></u><br>With Unlimited Access to Real&nbsp;Python</p>
    <p class="mb-2"><img loading="lazy" class="w-50 mb-2" src="/static/videos/lesson-locked.f5105cfd26db.svg" width="510" height="260"></p>
    <p class="mx-auto w-50 mb-3"><strong>Join us and get access to thousands of tutorials, hands-on video courses, and a community of expert Pythonistas:</strong></p>
    <p><a href="/account/join/?utm_source=rp_article_footer&utm_content=gradient-descent-algorithm-python" class="btn btn-primary btn-lg px-4">Level Up Your Python Skills »</a>
  </div>
</div>

  

  
  <div class="card mt-4" id="reader-comments">
    <p class="card-header h3">What Do You Think?</p>
    
    <div class="text-center mt-3 mb-0 p-0">
      <div class="mb-2">
        <strong class="mr-2">Rate this article:</strong>
        
<div class="btn-group mb-0">
  <a class="btn btn-secondary border-right" style="border-top-right-radius: 0; border-bottom-right-radius: 0;" title="Liked it" role="button" aria-label="Thumbs up (liked it)" href="/feedback/survey/article/gradient-descent-algorithm-python/liked/?from=article-comments" target="_blank"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#light--thumbs-up"/></svg></span></a>
  <a class="btn btn-secondary border-left" role="button" aria-label="Thumbs down (disliked it)" title="Disliked it" href="/feedback/survey/article/gradient-descent-algorithm-python/disliked/?from=article-comments" target="_blank"><span class="icon baseline"><svg><use href="/static/icons.0c2040efe47b.svg#light--thumbs-down"/></svg></span></a>
</div>

      </div>
      



<div class="text-center my-3">
  <span>
    
    
    <a target="_blank" rel="nofollow" href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Frealpython.com%2Fgradient-descent-algorithm-python%2F" class="mr-1 btn btn-sm badge-linkedin text-light mb-1"><span class="icon baseline mr-1 text-light"><svg><use href="/static/icons.0c2040efe47b.svg#brands--linkedin"/></svg></span>LinkedIn</a>
    
    
    <a target="_blank" rel="nofollow" href="https://twitter.com/intent/tweet/?text=Interesting%20Python%20article%20on%20%40realpython%3A%20Stochastic%20Gradient%20Descent%20Algorithm%20With%20Python%20and%20NumPy&url=https%3A%2F%2Frealpython.com%2Fgradient-descent-algorithm-python%2F" class="mr-1 btn btn-sm badge-x-twitter text-light mb-1"><span class="icon baseline mr-1 text-light"><svg><use href="/static/icons.0c2040efe47b.svg#brands--x-twitter"/></svg></span>Twitter</a>
    
    
    <a target="_blank" rel="nofollow" href="https://facebook.com/sharer/sharer.php?u=https%3A%2F%2Frealpython.com%2Fgradient-descent-algorithm-python%2F" class="mr-1 btn btn-sm badge-facebook text-light mb-1"><span class="icon baseline mr-1 text-light"><svg><use href="/static/icons.0c2040efe47b.svg#brands--facebook"/></svg></span>Facebook</a>
    
    
    <a target="_blank" rel="nofollow" href="mailto:?subject=Python%20article%20for%20you&body=Stochastic%20Gradient%20Descent%20Algorithm%20With%20Python%20and%20NumPy%20on%20Real%20Python%0A%0Ahttps%3A%2F%2Frealpython.com%2Fgradient-descent-algorithm-python%2F%0A" class="mr-1 btn btn-sm badge-dark text-light mb-1"><span class="icon baseline mr-1 text-light"><svg><use href="/static/icons.0c2040efe47b.svg#solid--envelope"/></svg></span>Email</a>
    
    
  </span>
</div>

    </div>
    
    <div class="card-body">
      <p>What’s your #1 takeaway or favorite thing you learned? How are you going to put your newfound skills to use? Leave a comment below and let us know.</p>

      <div class="alert alert-dark">
        <p class="mb-0"><strong>Commenting Tips:</strong> The most useful comments are those written with the goal of learning from or helping out other students. <a href="https://realpython.com/python-beginner-tips/#tip-9-ask-good-questions" target="_blank">Get tips for asking good questions</a> and <a href="https://support.realpython.com" target="_blank">get answers to common questions in our support portal</a>.<hr>Looking for a real-time conversation? Visit the <a href="/community/" target="_blank">Real Python Community Chat</a> or join the next <a href="/office-hours/" target="_blank">&ldquo;Office&nbsp;Hours&rdquo; Live Q&A Session</a>. Happy Pythoning!</p>

      </div>

      <div class="mb-4" id="disqus_thread">
      
      </div>
      
    </div>
  </div>

  <div class="card mt-4 mb-4">
    <p class="card-header h3">Keep Learning</p>
    <div class="card-body">
      <p class="mb-0">Related Tutorial Categories:
      
        <a href="/tutorials/advanced/" class="badge badge-light text-muted">advanced</a>
      
        <a href="/tutorials/machine-learning/" class="badge badge-light text-muted">machine-learning</a>
      
        <a href="/tutorials/numpy/" class="badge badge-light text-muted">numpy</a>
      
      </p>
      
    </div>
  </div>

  
    

<div class="modal fade" tabindex="-1" role="dialog" id="rprw">
  <div class="modal-dialog modal-lg modal-dialog-centered" role="document">
    <div class="modal-content">

      <div class="modal-header border-0">
        <div class="col-12 col-lg-9 mx-auto modal-title text-center my-2">
          <h2 class="my-0">Keep reading Real&nbsp;Python by creating a free account or signing&nbsp;in:</h2>
        </div>
      </div>

      
      <div class="modal-body bg-light">
        <div class="col-12 text-center">

          <div class="col-12 col-lg-8 mx-auto mb-2 mt-3">
            <a href="/account/signup/?intent=continue_reading&utm_source=rp&utm_medium=web&utm_campaign=rwn&utm_content=v1&next=%2Fgradient-descent-algorithm-python%2F"><img loading="lazy" class="w-100" src="/static/videos/lesson-locked.f5105cfd26db.svg"  width="510" height="260" alt="Keep reading"></a>
          </div>

          <p><a href="/account/signup/?intent=continue_reading&utm_source=rp&utm_medium=web&utm_campaign=rwn&utm_content=v1&next=%2Fgradient-descent-algorithm-python%2F" class="btn btn-primary btn-lg px-5"></i>Continue »</a></p>

        </div>
      </div>
      

      <div class="modal-footer border-0">
        <p class="text-center text-muted mt-2 mb-1">Already have an account? <a href="/account/login/?next=/gradient-descent-algorithm-python/">Sign-In</a></p>
      </div>

    </div>
  </div>
</div>

    <script src="/static/frontend/reader/rw.bea76ad0e23f.js" async></script>
  


    </div>

  </div>

    </div>

    
    

    
    
      
        
          <div class="modal fade" id="modal-python-mastery-course" tabindex="-1" aria-labelledby="modal-python-mastery-course-title" aria-describedby="modal-python-mastery-course-desc">
  <div class="modal-dialog modal-dialog-centered modal-lg">
    <div class="modal-content">

      <div class="modal-header bg-light pt-3 pb-2">
        <div class="container-fluid">
          <div class="row">
            <div class="col-12">
              <div class="progress" style="height: .5rem;" aria-hidden="true">
                <div class="progress-bar progress-bar-striped progress-bar-animated w-75" role="progressbar"></div>
              </div>
            </div>
            <div class="col-12">
              <p id="modal-python-mastery-course-desc" class="text-muted text-center mb-0 mt-2">Almost there! Complete this form and click the button below to gain instant&nbsp;access:</p>
            </div>
          </div>
        </div>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>

      <div class="modal-body m-4">
        <div class="container-fluid">
          <div class="row align-items-center text-center">
            <div class="col-12 col-lg-4 mb-4 mb-lg-0">
              
              
                <img loading="lazy" class="img-fluid rounded" src="https://files.realpython.com/media/python-logo.8eb72ea6927b.png" width="1000" height="1000" srcset="/cdn-cgi/image/width=250,format=auto/https://files.realpython.com/media/python-logo.8eb72ea6927b.png 250w, /cdn-cgi/image/width=333,format=auto/https://files.realpython.com/media/python-logo.8eb72ea6927b.png 333w, /cdn-cgi/image/width=500,format=auto/https://files.realpython.com/media/python-logo.8eb72ea6927b.png 500w, /cdn-cgi/image/width=1000,format=auto/https://files.realpython.com/media/python-logo.8eb72ea6927b.png 1000w" sizes="(min-width: 1000px) 207px, (min-width: 580px) 382px, calc(100vw - 134px)" alt="Python Logo">
              
            </div>
            <div class="col">
              <p id="modal-python-mastery-course-title" class="text-center h3 mb-4">5 Thoughts On Python Mastery</p>

              <form class="col-12" action="/optins/process/" method="post">
                <input type="hidden" name="csrfmiddlewaretoken" value="WMo7jZKy0VzLokBMAryCrBUj4p6InOrGDcUwE7VRo9NWcEXEivCM7yz9u9xak2EP">
                <input type="hidden" name="slug" value="python-mastery-course">
                <input type="hidden" name="source_path" value="/gradient-descent-algorithm-python/"> 
                <div class="form-group">
                  <input type="email" name="email" class="form-control" placeholder="Email Address" required autofocus>
                </div>
                <button name="submit" type="submit" class="btn btn-primary btn-block text-wrap">Start the Class »</button>
                <p class="mb-0 mt-2 text-muted text-center"><small>🔒 No spam. We take your privacy seriously.</small></p>
              </form>

            </div>
          </div>
        </div>
      </div>

    </div>
  </div>
</div>

        
      
    

    
      
<footer class="footer">
  <div class="container">
    
      <div class="mx-auto mt-4 mb-0" style="max-width: 768px;">
        

<div style="display:block;position:relative;">
  <div style="display:block;width:100%;padding-top:12.5%;"></div>
  <div class="rpad rounded border" data-unit="8x1" style="position:absolute;left:0;top:0;right:0;bottom:0;overflow:hidden;"></div>
</div>
<a class="small text-muted" href="/account/join/" rel="nofollow"><span class="icon baseline mr-1"><svg><use href="/static/icons.0c2040efe47b.svg#solid--circle-info"/></svg></span>Remove ads</a>


      </div>
    

    <p class="text-center text-muted w-75 mx-auto">© 2012–2023 Real&nbsp;Python&nbsp;⋅ <a href="/newsletter/">Newsletter</a>&nbsp;⋅ <a href="/podcasts/rpp/">Podcast</a>&nbsp;⋅ <a href="https://www.youtube.com/realpython">YouTube</a>&nbsp;⋅ <a href="https://twitter.com/realpython">Twitter</a>&nbsp;⋅ <a href="https://facebook.com/LearnRealPython">Facebook</a>&nbsp;⋅ <a href="https://www.instagram.com/realpython/">Instagram</a>&nbsp;⋅ <a href="/">Python&nbsp;Tutorials</a>&nbsp;⋅ <a href="/search">Search</a>&nbsp;⋅ <a href="/privacy-policy/">Privacy Policy</a>&nbsp;⋅ <a href="/energy-policy/">Energy Policy</a>&nbsp;⋅ <a href="/sponsorships/">Advertise</a>&nbsp;⋅ <a href="/contact/">Contact</a><br><span class="icon baseline text-red"><svg><use href="/static/icons.0c2040efe47b.svg#solid--heart"/></svg></span> Happy Pythoning!</p>
  </div>
</footer>

    

    
    
    <script>
      (function(document, history, location) {
        var HISTORY_SUPPORT = !!(history && history.pushState);

        var anchorScrolls = {
          ANCHOR_REGEX: /^#[^ ]+$/,
          OFFSET_HEIGHT_PX: 120,

          /**
           * Establish events, and fix initial scroll position if a hash is provided.
           */
          init: function() {
            this.scrollToCurrent();
            window.addEventListener('hashchange', this.scrollToCurrent.bind(this));
            document.body.addEventListener('click', this.delegateAnchors.bind(this));
          },

          /**
           * Return the offset amount to deduct from the normal scroll position.
           * Modify as appropriate to allow for dynamic calculations
           */
          getFixedOffset: function() {
            return this.OFFSET_HEIGHT_PX;
          },

          /**
           * If the provided href is an anchor which resolves to an element on the
           * page, scroll to it.
           * @param  {String} href
           * @return {Boolean} - Was the href an anchor.
           */
          scrollIfAnchor: function(href, pushToHistory) {
            var match, rect, anchorOffset;

            if(!this.ANCHOR_REGEX.test(href)) {
              return false;
            }

            match = document.getElementById(href.slice(1));

            if(match) {
              rect = match.getBoundingClientRect();
              anchorOffset = window.pageYOffset + rect.top - this.getFixedOffset();
              window.scrollTo(window.pageXOffset, anchorOffset);

              // Add the state to history as-per normal anchor links
              if(HISTORY_SUPPORT && pushToHistory) {
                history.pushState({}, document.title, location.pathname + href);
              }
            }

            return !!match;
          },

          /**
           * Attempt to scroll to the current location's hash.
           */
          scrollToCurrent: function() {
            this.scrollIfAnchor(window.location.hash);
          },

          /**
           * If the click event's target was an anchor, fix the scroll position.
           */
          delegateAnchors: function(e) {
            var elem = e.target;

            // 
            if (elem.dataset.toggle === "tab") {
              return;
            }

            if(
              elem.nodeName === 'A' &&
              this.scrollIfAnchor(elem.getAttribute('href'), true)
            ) {
              e.preventDefault();
            }
          }
        };

        window.addEventListener(
          'DOMContentLoaded', anchorScrolls.init.bind(anchorScrolls)
        );
      })(window.document, window.history, window.location);
    </script>
    

    
    <script>
      (function() {
        var isAndroid = navigator.userAgent.toLowerCase().indexOf("android") > -1;
        if (!isAndroid) {
          return;
        }

        var styles = `
        @font-face {
          font-family: 'DejaVu Sans Mono';
          font-weight: normal;
          font-style: normal;
          font-display: swap;
          src: url('/static/mfonts/dejavu-sans-mono.33f00225f915.woff2') format('woff2'),
               url('/static/mfonts/dejavu-sans-mono.0da77d3739f3.woff') format('woff'),
               url('/static/mfonts/dejavu-sans-mono.c2356fc49835.ttf') format('truetype');
        }
        code, kbd, pre, samp {
          font-family: 'DejaVu Sans Mono', monospace;
        }
        `

        var styleSheet = document.createElement("style")
        styleSheet.type = "text/css"
        styleSheet.innerText = styles
        document.head.appendChild(styleSheet)
      })();
    </script>

    
    
    <script src="/static/jquery.min.00727d1d5d9c.js"></script>
    <script src="/static/popper.min.47dc3aaf2942.js"></script>
    <script src="/static/bootstrap.min.a3b2689424c3.js"></script>

    
    

    <script>
    (function() {
      document.querySelectorAll(".js-search-form-submit").forEach(function(el) {
        el.addEventListener("click", function(e) {
          e.preventDefault();
          e.currentTarget.parentElement.submit();
        })
      });
    })();
    </script>
    <script src="/static/frontend/reader/codeblock.5c8399b698d9.js" async></script>
    <script src="/static/frontend/reader/lightbox.45ed8c5a3d27.js" async></script>
    <script src="/static/frontend/reader/platforms-ui.b11202dc6079.js" async></script>
    

    

    
    

    

    
    

    

<script>window.rp_prop_id = '58946116052';</script>
<script src="https://srv.realpython.net/tag.js" async></script>



<script src="/static/frontend/reader/toc-refresh.f019e7905d90.js" async></script>

<script id="js-context" type="application/json">{"is_completed": false, "is_bookmarked": false, "api_article_bookmark_url": "/api/v1/articles/gradient-descent-algorithm-python/bookmark/", "api_article_completion_status_url": "/api/v1/articles/gradient-descent-algorithm-python/completion_status/"}</script>
<script src="/static/frontend/reader/completion-status.dd9ca59c605f.js" async></script>






  
    <script id="dsq-count-scr" src="https://realpython.disqus.com/count.js" async></script>
    <script>
      var disqus_config = function () {
        this.page.url = 'https://realpython.com/gradient-descent-algorithm-python/';
        this.page.identifier = 'https://realpython.com/gradient-descent-algorithm-python/';
        this.callbacks.onReady = [function() {
          if (window.onDisqusReady) {
            window.onDisqusReady();
          }
        }];
      };
      var disqus_script_url = 'https://realpython.disqus.com/embed.js';
    </script>
    <script src="/static/frontend/reader/lazy-disqus.07ee9079f4a3.js" defer></script>
  


  
  
  <script src="https://cdn.onesignal.com/sdks/OneSignalSDK.js" async></script>
  <script>
    var OneSignal = window.OneSignal || [];
    OneSignal.push(function() {
      OneSignal.init({
        appId: "c0081e20-a523-42bb-b0ac-04c5a9e8bf40"
      });
    });
  </script>
  

  
  <script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "Article",
    "headline": "Stochastic Gradient Descent Algorithm With Python and NumPy",
    
    "image": {
      "@type": "ImageObject",
      "url": "https://files.realpython.com/media/Learning-How-Neural-Networks-Learn-Stochastic-Gradient-Descent-Algorithm-With-Python-and-NumPy_Watermarked.140a53be18a9.jpg",
      "width": 1920,
      "height": 1080
    },
    
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://realpython.com/gradient-descent-algorithm-python/"
    },
    "datePublished": "2021-01-27T14:00:00+00:00",
    "dateModified": "2023-10-21T02:58:30.810487+00:00",
     "publisher": {
      "@type": "Organization",
      "name": "Real Python",
      "logo": {
        "@type": "ImageObject",
        "url": "/static/real-python-logo-square-tiny.b2452b6d3823.png",
        "width": 60,
        "height": 60
      }
    },
    "author": {
      "@type": "Organization",
      "name": "Real Python",
      "url": "https://realpython.com",
      "logo": "/static/real-python-logo-square.146e987bf77c.png"
    },
    "description": "In this tutorial, you\u0027ll learn what the stochastic gradient descent algorithm is, how it works, and how to implement it with Python and NumPy."
  }
  </script>
  


    
    <script src="/static/frontend/search/autocomplete.95e5945849f5.js" async></script>
    

    

    

    

    

    
<script>
  !function(f,b,e,v,n,t,s)
  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
  n.callMethod.apply(n,arguments):n.queue.push(arguments)};
  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
  n.queue=[];t=b.createElement(e);t.async=!0;
  t.src=v;s=b.getElementsByTagName(e)[0];
  s.parentNode.insertBefore(t,s)}(window, document,'script',
  'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '2220911568135371');
  fbq('track', 'PageView');
</script>
<noscript><img height="1" width="1" style="display:none"
  src="https://www.facebook.com/tr?id=2220911568135371&ev=PageView&noscript=1"
/></noscript>


    

    

  <script>(function(){var js = "window['__CF$cv$params']={r:'82f6626cbd610716',t:'MTcwMTU0OTYyMS41MzkwMDA='};_cpo=document.createElement('script');_cpo.nonce='',_cpo.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js',document.getElementsByTagName('head')[0].appendChild(_cpo);";var _0xh = document.createElement('iframe');_0xh.height = 1;_0xh.width = 1;_0xh.style.position = 'absolute';_0xh.style.top = 0;_0xh.style.left = 0;_0xh.style.border = 'none';_0xh.style.visibility = 'hidden';document.body.appendChild(_0xh);function handler() {var _0xi = _0xh.contentDocument || _0xh.contentWindow.document;if (_0xi) {var _0xj = _0xi.createElement('script');_0xj.innerHTML = js;_0xi.getElementsByTagName('head')[0].appendChild(_0xj);}}if (document.readyState !== 'loading') {handler();} else if (window.addEventListener) {document.addEventListener('DOMContentLoaded', handler);} else {var prev = document.onreadystatechange || function () {};document.onreadystatechange = function (e) {prev(e);if (document.readyState !== 'loading') {document.onreadystatechange = prev;handler();}};}})();</script></body>
</html>
