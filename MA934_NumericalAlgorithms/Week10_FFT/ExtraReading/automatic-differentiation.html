<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Automatic Differentiation for Deep Learning, by example | Connecting deep dots</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Automatic Differentiation for Deep Learning, by example" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="TL;DR In essence, neural networks are simply mathematical functions that are composed of many simpler functions. During training, we need to find partial derivatives of each weight (or bias) at a specific weight setting to make adjustments. All partial derivatives together are called the gradient (vector) and boil down to real numbers for a specific input to the function. This calculation can be easily programmed using reverse mode automatic differentiation which powers numerical frameworks such as TensorFlow or PyTorch. Let’s peek under the hood and work out a couple of concrete examples (including a small Numpy implementation) to see the magic and connect the dots!" />
<meta property="og:description" content="TL;DR In essence, neural networks are simply mathematical functions that are composed of many simpler functions. During training, we need to find partial derivatives of each weight (or bias) at a specific weight setting to make adjustments. All partial derivatives together are called the gradient (vector) and boil down to real numbers for a specific input to the function. This calculation can be easily programmed using reverse mode automatic differentiation which powers numerical frameworks such as TensorFlow or PyTorch. Let’s peek under the hood and work out a couple of concrete examples (including a small Numpy implementation) to see the magic and connect the dots!" />
<link rel="canonical" href="https://alexander-schiendorfer.github.io/2020/02/16/automatic-differentiation.html" />
<meta property="og:url" content="https://alexander-schiendorfer.github.io/2020/02/16/automatic-differentiation.html" />
<meta property="og:site_name" content="Connecting deep dots" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-02-16T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Automatic Differentiation for Deep Learning, by example" />
<script type="application/ld+json">
{"headline":"Automatic Differentiation for Deep Learning, by example","dateModified":"2020-02-16T00:00:00+00:00","datePublished":"2020-02-16T00:00:00+00:00","description":"TL;DR In essence, neural networks are simply mathematical functions that are composed of many simpler functions. During training, we need to find partial derivatives of each weight (or bias) at a specific weight setting to make adjustments. All partial derivatives together are called the gradient (vector) and boil down to real numbers for a specific input to the function. This calculation can be easily programmed using reverse mode automatic differentiation which powers numerical frameworks such as TensorFlow or PyTorch. Let’s peek under the hood and work out a couple of concrete examples (including a small Numpy implementation) to see the magic and connect the dots!","url":"https://alexander-schiendorfer.github.io/2020/02/16/automatic-differentiation.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://alexander-schiendorfer.github.io/2020/02/16/automatic-differentiation.html"},"@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
  <link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://alexander-schiendorfer.github.io/feed.xml" title="Connecting deep dots" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
          ]}
        );
      });
    </script>
  
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Connecting deep dots</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about.html">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Automatic Differentiation for Deep Learning, by example</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-02-16T00:00:00+00:00" itemprop="datePublished">Feb 16, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="tldr">TL;DR</h2>
<p>In essence, neural networks are simply mathematical functions that are composed of many simpler functions. During training, we need to find partial derivatives of each weight (or bias) at a specific weight setting to make adjustments. All partial derivatives together are called the <em>gradient</em> (vector) and boil down to real numbers for a specific input to the function. This calculation can be easily programmed using <em>reverse mode automatic differentiation</em> which powers numerical frameworks such as <a href="https://www.tensorflow.org/tutorials/customization/autodiff">TensorFlow</a> or <a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html">PyTorch</a>. Let’s peek under the hood and work out a couple of concrete examples (including a small Numpy implementation) to see the magic and connect the dots!</p>

<p>Find a jupyter notebook accompanying the post <a href="https://github.com/Alexander-Schiendorfer/Alexander-Schiendorfer.github.io/blob/master/notebooks/simple-autodiff.ipynb">here</a> or directly in Colab:</p>

<table align="left">
  <td>
    <a target="_blank" href="https://colab.research.google.com/github/Alexander-Schiendorfer/Alexander-Schiendorfer.github.io/blob/master/notebooks/simple-autodiff.ipynb"><img src="https://www.tensorflow.org/images/colab_logo_32px.png" />Run in Google Colab</a>
  </td>
</table>
<h2 id="understanding-the-chain-rule">Understanding the Chain Rule</h2>
<p>Neural networks calculate their output (for a given input) by repeatedly multiplying features by weights, summing them up, and applying activation functions to obtain non-linear mappings.</p>

<p><img src="/images/backprop/nnet-vanilla.gif" alt="A simple forward pass in a neural network" /></p>

<!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/CUkQor4Wyes" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->

<p>Especially in very deep networks (e.g., ResNet152 has 152 layers!), calculating the partial derivatives for the weight updates seems daunting and error-prone to do manually.</p>

<p><img src="/images/backprop/deepnet.png" alt="Deep neural networks" /></p>

<p>At this point of studying backpropagation, I was severely nervous about taking all those derivatives by hand … Fortunately, this tedious procedure is a mechanical operation that we can easily <a href="https://arxiv.org/abs/1502.05767">automate</a> - at least for certain concrete values of a function (as opposed to getting a nice and clean symbolic function such as “$2x$” for “$x^2$” that we do not even need for training neural networks)!</p>

<div class="Toast Toast--warning">

   <span class="Toast-icon"><svg class="octicon octicon-alert" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.22 1.754a.25.25 0 00-.44 0L1.698 13.132a.25.25 0 00.22.368h12.164a.25.25 0 00.22-.368L8.22 1.754zm-1.763-.707c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0114.082 15H1.918a1.75 1.75 0 01-1.543-2.575L6.457 1.047zM9 11a1 1 0 11-2 0 1 1 0 012 0zm-.25-5.25a.75.75 0 00-1.5 0v2.5a.75.75 0 001.5 0v-2.5z"></path></svg></span>
   
   <span class="Toast-content">
A major source of confusion to me was the "data type" of <i>a gradient</i> or the partial derivatives, for that matter. In my mind, the meaning of <code>df / dt</code>
 was a 'function', as in "the function over all of its domain", perhaps in symbolic form. But here, we refer to <i>specific value of the gradient</i> at a <i>specific point</i>, i.e., a set of weights with concrete values, say <code>[0.4, 1.2]</code>. Because that's really all we need for training!
   </span>
</div>

<p>Since neural networks chain together several weighted sums and activation functions we need to revisit the chain rule (again, <a href="https://www.youtube.com/watch?v=YG15m2VwSjA">3Blue1Brown</a> saves the day!). We are moving from neural networks such as $f(\vec{w}; \vec{x}) = \max ( \sum_{i = 1}^k w_i \cdot x_i, 0)$ to much simpler functions, for the moment. A very helpful picture to visualize is that of a <em>computational graph</em> as a circuit of functions with input and output values that are chained together, as proposed by <a href="http://cs231n.github.io/optimization-2/">CS231n</a>.</p>

<p><img src="/images/backprop/chainbasic.png" alt="A small function for the chain rule" /></p>

<p>Or in code:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">*</span><span class="n">y</span>

<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">h</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">h</span><span class="o">**</span><span class="mi">2</span>

<span class="n">h</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">o</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
</code></pre></div></div>

<p>The input values $x$ and $y$ are fed into the first function $f$ (which just multiplies them together) to form the intermediate variable $h$ which is then fed into $g$ to form the output $o$. Let’s apply this to the inputs 3 and 2:</p>

<p><img src="/images/backprop/chainvalues.png" alt="A small function for the chain rule with values 3 and 2" /></p>

<p>We can view $o$ as a function of $x$ and $y$ which makes it natural to ask for its partial derivatives, $\frac{\partial o}{\partial x}$ and $\frac{\partial o}{\partial y}$. This perspective gives a very intuitive interpretation of the chain rule. Being a bit sloppy here (mathematically), think of the partial derivative $\frac{\partial o}{\partial h}$ as the ratio between the change in $o$ that a little nudge to $h$ provokes.</p>

<p>Let’s work this out in isolation for both functions. Consider $x = 3$ and $y = 2$. Differentiating according to $x$ yields $\frac{\partial h}{\partial x} = y$, for the concrete case 2. That means that a little increment to $x$ such as 0.01 would provoke a change by a factor 2, so $h$ would increase by 0.02.</p>

<p>Similarly, for $h = 6$ the derivative of $g(h) = h^2$ (of course, with respect to $h$) yields $2h$, 12 for our example. Hence, increasing $h$ by 0.01 would cause an increase by 0.12 in $o$. Now just chain these two together: A little increase $\Delta$ in $x$ will trigger a $2\Delta$ increase in $h$. And since every $\Delta$ increase in $h$ causes a $12\Delta$, the total increase in $o$ when we increase $x$ by $\Delta$ is simply $2 \cdot 12 \cdot \Delta$, i.e., a factor of $24$. Although this thought is not formally bullet-proof (we’re dealing with limits and such) it helped me a lot to form intuition and the arithmetic of the chain rule is consistent with it:</p>

\[\frac{\partial o}{\partial x} = \frac{\partial o}{\partial h} \cdot \frac{\partial h}{\partial x}\]

<p>which we can again program exactly for given values of $x$ and $y$:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span><span class="mi">2</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> 
<span class="n">o</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>

<span class="c1"># first the partial derivative d o / d h
</span><span class="n">do_dh</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">h</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mi">12</span>
<span class="n">dh_dx</span> <span class="o">=</span> <span class="n">y</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mi">2</span> 
<span class="c1"># then the chain rule application
</span><span class="n">do_dx</span> <span class="o">=</span> <span class="n">do_dh</span> <span class="o">*</span> <span class="n">dh_dx</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mi">24</span>
</code></pre></div></div>
<p>which we can use to fill in our graph diagram:</p>

<p><img src="/images/backprop/chainbackx.png" alt="A small function for the chain rule with values 3 and 2" /></p>

<p>If we apply the chain rule for $y$, we get an analogous result:</p>

\[\frac{\partial o}{\partial y} = \frac{\partial o}{\partial h} \cdot \frac{\partial h}{\partial y}\]

<p><img src="/images/backprop/chainbackxy.png" alt="A small function for the chain rule with values 3 and 2" /></p>

<p>Note that the partial derivative $\frac{\partial o}{\partial y}$ is simply larger because we multiply $\frac{\partial o}{\partial h} = 12$ by $\frac{\partial h}{\partial y} = 3$, every increase in $y$ is multiplied by $x = 3$.</p>

<p><strong>But, more importantly, note that $\frac{\partial o}{\partial h} = 12$ appears in both calculations!</strong></p>

<p>That is a key observation that is also exploited in backpropagation for neural networks: We can reuse derivatives for shared subexpressions in the gradient calculation, if we schedule the calculations correctly: Work out derivatives of the later layers before those of the earlier layers and reuse the results. Doing that in the right order is called the <em>backward pass</em> of a neural network, for apparent reasons:</p>

<video src="/images/backprop/forwardbackward.mp4" poster="/images/backprop/chainbackxy.png" width="100%" controls="" preload=""></video>

<p>The calculation needs the intermediate values (here, $h$) to calculate the partial derivatives and has to calculate them in the reverse order of the forward computation (this is why this method is called <code class="language-plaintext highlighter-rouge">reverse mode automatic differentiation</code>).</p>

<p>We can also look at somewhat more entangled graphs:</p>

<p><img src="/images/backprop/chaindiamond.png" alt="A small function for the chain rule with values 3 and 2 and 1 with a diamond" /></p>

<p>where, again, every subexpression gets their own name. Go ahead and try to perform forward and backward calculation using the chain rule!</p>

<!-- <video src="/images/backprop/graphdiamond.mp4" poster="/images/backprop/chaindiamond.png" width="100%"  controls ></video> -->
<iframe width="560" height="315" src="https://www.youtube.com/embed/hUXpIkhVhSY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p>You might have noticed that the variable $y$ affects the output $o$ via two paths: Once when calculating $h_1$, once for $h_2$. Fortunately, it suffices to sum up <em>all effects</em> that $y$ has to get the overall effect on $o$. <a href="https://en.wikipedia.org/wiki/Chain_rule#Multivariable_case">Mathematicians</a> call this the <em>multivariable chain rule</em>. We need this in neural networks since changing a weight in early layers affects multiple hidden units and outputs - and finally the loss.</p>

<p>Now we observe a pattern that makes the whole process look very algorithmic, i.e., easy to automate for a computer. Simple functions are chained together to form more complex functions and we can calculate all partial derivatives in one go by simple <em>reversing</em> the order of operations. Notice how all we needed to do to calculate these derivatives were just a couple of multiplications and additions - this is what makes GPUs so good for training neural nets! I think that is really cool.</p>

<h3 id="dynamic-computation-graphs-and-automatic-differentiation">Dynamic computation graphs and automatic differentiation</h3>
<p>A single computational node (e.g., for multiplying or adding or taking the square) only has to perform the operation, store its output and know how to take the <em>local derivative</em>, given the derivative of its output. Let’s zoom in on such a guy:</p>

<p><img src="/images/backprop/compnode.gif" alt="Focus on a single computational node" /></p>

<p>For now, we only care about scalar functions (i.e., a single number) as the output of our computational graph. To calculate the partial derivatives, we have to do the local chain-rule updates in the <em>reverse order</em> of the forward calculations. Every node thus gets a <em>gradient</em> attribute for its output (the red numbers). That way, we can be sure that any “upstream gradient” is calculated before we need it in downstream nodes.</p>

<p>A general solution would be to calculate the graph in forward order and then perform <em><a href="https://en.wikipedia.org/wiki/Topological_sorting">topological sorting</a></em> to find an appropriate linear ordering. Conceptually even simpler are <em><a href="https://www.tensorflow.org/tutorials/customization/autodiff">gradient tapes</a></em>. 
We might think of keeping a “log” like this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#1: h1 = Multiply(3,2)
#2: h2 = Multiply(2,1)
#3: h = Multiply(h1, h2)
#4: o = Square(h)
</code></pre></div></div>
<p>and reversing the order of operations when calculating the derivatives</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#4: grad o = Square(h)
#3: grad h = Multiply(h1, h2)
#2: grad h2 = Multiply(2,1)
#1: grad h1 = Multiply(3,2)
</code></pre></div></div>

<p>Our implementation revolves around the base class <code class="language-plaintext highlighter-rouge">CompNode</code> from which all our concrete functions  (multiplication, squaring, adding, etc.) would inherit (an example of the <a href="https://en.wikipedia.org/wiki/Composite_pattern">composite pattern</a>):</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CompNode</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tape</span><span class="p">):</span>
        <span class="c1"># make sure that the gradient tape knows us
</span>        <span class="n">tape</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    
    <span class="c1"># perform the intended operation 
</span>    <span class="c1"># and store the result in self.output
</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
    
    <span class="c1"># assume that self.gradient has all the information 
</span>    <span class="c1"># from outgoing nodes prior to calling backward
</span>    <span class="c1"># -&gt; perform the local gradient step with respect to inputs
</span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
    
    <span class="c1"># needed to be initialized to 0 
</span>    <span class="k">def</span> <span class="nf">set_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradient</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gradient</span> <span class="o">=</span> <span class="n">gradient</span>
        
    <span class="c1"># receive gradients from downstream nodes     
</span>    <span class="k">def</span> <span class="nf">add_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradient</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gradient</span> <span class="o">+=</span> <span class="n">gradient</span>
</code></pre></div></div>

<p>The gradient tape does little more than expect computation nodes, store their order of operations,
and call <code class="language-plaintext highlighter-rouge">backward</code> on every computation node in the reverse order:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Tape</span><span class="p">:</span>    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">computations</span> <span class="o">=</span> <span class="p">[]</span>
        
    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">compNode</span> <span class="p">:</span> <span class="n">CompNode</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">computations</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">compNode</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">compNode</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">computations</span><span class="p">:</span>
            <span class="n">compNode</span><span class="p">.</span><span class="n">forward</span><span class="p">()</span>
            
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># first initialize all gradients to zero 
</span>        <span class="k">for</span> <span class="n">compNode</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">computations</span><span class="p">:</span>
            <span class="n">compNode</span><span class="p">.</span><span class="n">set_gradient</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            
        <span class="c1"># we need to invert the order    
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">computations</span><span class="p">.</span><span class="n">reverse</span><span class="p">()</span>    
        <span class="c1"># last node gets a default value of one for the gradient
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">computations</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_gradient</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">compNode</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">computations</span><span class="p">:</span>
            <span class="n">compNode</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre></div></div>

<p>The last node (our overall scalar output) receives a gradient of one (since $\frac{\partial f}{\partial f} = 1$).</p>

<p>A particular simple node is that representing a constant value.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ConstantNode</span><span class="p">(</span><span class="n">CompNode</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">tape</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="n">tape</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">value</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># nothing to do here
</span>        <span class="k">pass</span>
</code></pre></div></div>

<p>which we can use for <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t</span> <span class="o">=</span> <span class="n">Tape</span><span class="p">()</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">ConstantNode</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">t</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">ConstantNode</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="n">t</span><span class="p">)</span>
</code></pre></div></div>

<p>The multiplication operation is rather straightforward to implement:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Multiply</span><span class="p">(</span><span class="n">CompNode</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">left</span> <span class="p">:</span> <span class="n">CompNode</span><span class="p">,</span> <span class="n">right</span> <span class="p">:</span> <span class="n">CompNode</span><span class="p">,</span> <span class="n">tape</span> <span class="p">:</span> <span class="n">Tape</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">left</span> <span class="o">=</span> <span class="n">left</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">right</span> <span class="o">=</span> <span class="n">right</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">left</span><span class="p">.</span><span class="n">output</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">right</span><span class="p">.</span><span class="n">output</span>
        
    <span class="c1"># has to know how to locally differentiate multiplication
</span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">left</span><span class="p">.</span><span class="n">add_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">right</span><span class="p">.</span><span class="n">output</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">gradient</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">right</span><span class="p">.</span><span class="n">add_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">left</span><span class="p">.</span><span class="n">output</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">gradient</span><span class="p">)</span>
</code></pre></div></div>
<p>Note that it stores references to its <em>inputs</em> (<code class="language-plaintext highlighter-rouge">left</code> and <code class="language-plaintext highlighter-rouge">right</code>) in order to inform them about gradients.
In the forward pass, it simply multiplies the outputs of both input nodes and stores this as its own output. In the backward pass, it multiplies each input’s gradient with the <em>opposite</em> input’s output. Is that a bug?</p>

<p>No, it’s simply the local gradient for multiplication: if $f(a,b) = a \cdot b$ then $\frac{\partial f}{\partial a} = b$ and $\frac{\partial f}{\partial b} = a$ and those are exactly the local gradients we need!</p>

<p>We are now ready to automatically differentiate our previous example:</p>

<p><img src="/images/backprop/compnode_full.png" alt="A single computational node" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t</span> <span class="o">=</span> <span class="n">Tape</span><span class="p">()</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">ConstantNode</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">t</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">ConstantNode</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="n">t</span><span class="p">)</span>

<span class="n">o</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">(</span><span class="n">ConstantNode</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">t</span><span class="p">),</span> <span class="n">o</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="n">t</span><span class="p">.</span><span class="n">forward</span><span class="p">()</span>
</code></pre></div></div>

<p>Calling <code class="language-plaintext highlighter-rouge">backward</code> on the tape will trigger the reverse-mode automatic differentiation. Some people call already that step backpropagation which I would reserve for the application of autodiff to neural networks and applying a gradient update on the weights.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">o</span><span class="p">.</span><span class="n">gradient</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mi">5</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">gradient</span><span class="p">)</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="mi">15</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">gradient</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mi">10</span>
</code></pre></div></div>

<p>Equipped with multiplication alone, we can even address our previous diamond-shaped graph:</p>

<p><img src="/images/backprop/chaindiamond_full.png" alt="A small function for the chain rule with values 3 and 2 and 1 with a diamond" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t</span> <span class="o">=</span> <span class="n">Tape</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">ConstantNode</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="n">t</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">ConstantNode</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">t</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">ConstantNode</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">t</span><span class="p">)</span>

<span class="n">h1</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="n">h2</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

<span class="n">h</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">(</span><span class="n">h1</span><span class="p">,</span> <span class="n">h2</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="n">o</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="n">t</span><span class="p">.</span><span class="n">forward</span><span class="p">()</span>
</code></pre></div></div>

<p>which replaced squaring by multiplication with itself.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">h</span><span class="p">.</span><span class="n">gradient</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mi">24</span> 
<span class="k">print</span><span class="p">(</span><span class="n">h1</span><span class="p">.</span><span class="n">gradient</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mi">48</span>
<span class="k">print</span><span class="p">(</span><span class="n">h2</span><span class="p">.</span><span class="n">gradient</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mi">144</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">gradient</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mi">96</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">gradient</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mi">288</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">.</span><span class="n">gradient</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mi">288</span>
</code></pre></div></div>

<p>How would you implement the squaring operation ($x^2$) more explicitly? The forward operation is rather obvious: we have one input and take the square as the node’s output. What about the backward operation? If our input is $x$ and our output is $f(x) = x^2$ then the local gradient is simply $\frac{\partial f}{\partial x} = 2\cdot x$ where $x$ is the input to the node.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Square</span><span class="p">(</span><span class="n">CompNode</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span> <span class="p">:</span> <span class="n">CompNode</span><span class="p">,</span> <span class="n">tape</span> <span class="p">:</span> <span class="n">Tape</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">.</span><span class="n">output</span><span class="o">**</span><span class="mi">2</span>
        
    <span class="c1"># has to know how to locally differentiate x^2
</span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">.</span><span class="n">add_gradient</span><span class="p">(</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">.</span><span class="n">output</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">gradient</span><span class="p">)</span>
</code></pre></div></div>
<p>and applying it is then rather straightforward:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t</span> <span class="o">=</span> <span class="n">Tape</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">ConstantNode</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="n">t</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">ConstantNode</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">t</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">ConstantNode</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">t</span><span class="p">)</span>

<span class="n">h1</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="n">h2</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

<span class="n">h</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">(</span><span class="n">h1</span><span class="p">,</span> <span class="n">h2</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="n">o</span> <span class="o">=</span> <span class="n">Square</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="n">t</span><span class="p">.</span><span class="n">forward</span><span class="p">()</span>
</code></pre></div></div>
<p>Verify for yourself that this yields the same gradients! Why would you even do this if you can express squaring by multiplication? Well, sometimes the gradient expressions are substantially simplified algebraically, as is the case for the sigmoid function $f(z) = \frac{1}{1 + e^{-z}}$. It has a nice local derivative $\frac{\partial f}{\partial z} = f(z) \cdot (1 - f(z))$ which can be expressed only in terms of the node’s output. Or you could implement every step using a primitive operation (see <a href="http://cs231n.github.io/optimization-2/#sigmoid">CS231n course notes</a> for an example). Your call! The general heuristic is “if there is an algebraically nice derivative (sigmoid, softmax), implement a <code class="language-plaintext highlighter-rouge">CompNode</code> for it, otherwise just let the framework work it out”.</p>

<p>As an exercise, try to implement the <code class="language-plaintext highlighter-rouge">Add</code> operation for simple addition. You can imagine building up an extensible autodiff framework with functions as building blocks that come with their own logic to differentiate – that’s precisely what deep learning frameworks do (among other cool things such as GPU-support, distributed execution, and pre-defined higher-level abstractions such as “layers” for neural nets)!</p>

<h1 id="but-why-dont-tensorflow-or-pytorch-look-so-complicated">But why don’t TensorFlow or PyTorch look so complicated?</h1>
<p>Admittedly, the way we wrote down reverse-mode autodiff with a gradient tape looks very tedious and unintuitive. You have all these objects for calculations and have to chain them together in the right way instead of writing arithmetic expressions such as:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">o</span> <span class="o">=</span> <span class="p">(</span> <span class="p">(</span><span class="n">x</span><span class="o">*</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">z</span><span class="p">)</span> <span class="p">)</span><span class="o">**</span><span class="mi">2</span>
</code></pre></div></div>
<p>What if I told you you can have the cake and eat it, too? The key ingredient here is <em><a href="https://en.wikipedia.org/wiki/Operator_overloading">operator overloading</a></em>.</p>

<p>When you write</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a * b 
</code></pre></div></div>
<p>the return type of this expression (and even the function that works on <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code>) depends on the types of the arguments. If <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code> are normal <code class="language-plaintext highlighter-rouge">int</code> values, standard multiplication is used. If <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code> are instances of <code class="language-plaintext highlighter-rouge">CompNode</code> we could go on and define or own version of <code class="language-plaintext highlighter-rouge">*</code>, e.g., one that takes both arguments to the constructor of our class <code class="language-plaintext highlighter-rouge">Multiply</code>. But be aware that you’re essentially <em>defining</em> calculations at this point (in PyTorch, you also get the forward pass immediately) instead of only executing them!</p>

<p>This helps to build a framework that has a nice frontend language to define calculations but handle the bookkeeping (wiring of operations, scheduling backward passes) in the background, without your noticing.  Pretty neat, eh?</p>

<p>Under the hood, that is what powers <a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html">PyTorch</a> or TensorFlow:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">GradientTape</span><span class="p">(</span><span class="n">persistent</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">t</span><span class="p">:</span>
  <span class="n">t</span><span class="p">.</span><span class="n">watch</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>
  <span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">y</span>
<span class="n">dz_dx</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># 108.0 (4*x^3 at x = 3)
</span><span class="n">dy_dx</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># 6.0
</span><span class="k">del</span> <span class="n">t</span>  <span class="c1"># Drop the reference to the tape
</span></code></pre></div></div>
<h1 id="connecting-dots-with-neural-networks">Connecting dots with Neural Networks</h1>

<p>Well, we have worked through an algorithmic approach to differentiating composed functions. Essentially this saves us trouble when applying the chain rule by calculating intermediate derivatives that we need for the derivatives we actually care about.</p>

<p><strong>But what does that have to do with a neural network?</strong></p>

<p>Consider a single training instance $(\vec{x}, \vec{t})$ where $\vec{x} =  [x_1, x_2, \ldots, x_m]$ and $\vec{t} = [t_1, \ldots, t_d]$. Our neural network transforms $\vec{x}$ to some output $\vec{y} = [y_1, \ldots, y_k]$ that can be compared to the target (what the network <em>should produce</em>) using its <em>current set of weights</em>. Let’s make this more explicit:</p>

<p><img src="/images/backprop/graphfornetwork_forward.png" alt="Applying reverse-mode autodiff to the loss function of a small network" /></p>

<p>For the inputs $\vec{x} = (2,3)$, the network with its current weights $0.4$ and $-0.2$ produces a scalar output $y$ using the sigmoid activation function. It outputs $0.55$ but we would like that to be closer to $1$ (the target $t$). As loss function, the squared error measures how far we are off. We can implement this graph using a few additional <code class="language-plaintext highlighter-rouge">CompNode</code> extensions (that you can find in the <a href="https://github.com/Alexander-Schiendorfer/Alexander-Schiendorfer.github.io/blob/master/notebooks/simple-autodiff.ipynb">notebook</a>).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gt</span> <span class="o">=</span> <span class="n">Tape</span><span class="p">()</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">ConstantNode</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span><span class="n">gt</span><span class="p">)</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">ConstantNode</span><span class="p">(</span><span class="mf">0.4</span><span class="p">,</span><span class="n">gt</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">ConstantNode</span><span class="p">(</span><span class="mf">3.</span><span class="p">,</span><span class="n">gt</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">ConstantNode</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span><span class="n">gt</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">ConstantNode</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="n">gt</span><span class="p">)</span>

<span class="n">h1</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">gt</span><span class="p">)</span>
<span class="n">h2</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">gt</span><span class="p">)</span>

<span class="n">h</span> <span class="o">=</span> <span class="n">Add</span><span class="p">(</span><span class="n">h1</span><span class="p">,</span> <span class="n">h2</span><span class="p">,</span> <span class="n">gt</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Sigmoid</span><span class="p">(</span><span class="n">h</span><span class="p">,</span><span class="n">gt</span><span class="p">)</span>

<span class="n">t_inv</span> <span class="o">=</span> <span class="n">Invert</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">gt</span><span class="p">)</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">Add</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t_inv</span><span class="p">,</span> <span class="n">gt</span><span class="p">)</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">Square</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">gt</span><span class="p">)</span>
<span class="n">gt</span><span class="p">.</span><span class="n">forward</span><span class="p">()</span>
</code></pre></div></div>
<p>This will give us all the gradients with respect to the loss function (for this single training instance):</p>

<p><img src="/images/backprop/graphfornetwork_full.png" alt="Applying reverse-mode autodiff to the loss function of a small network" /></p>

<p>We also found gradients for the inputs that we cannot really change. It does not make sense to adapt the target $t$ to what would make the loss smaller (that’s a bit cheating !?), we need to change the output $y$. Similarly, we cannot change the inputs $\vec{x}$ (except for when we explicitly look for <em>adversarial</em> inputs that fool our network). But the weight gradients reveal some interesting aspects. The weight gradients are negative. That means that the loss is likely to go down if we <em>increase</em> the weight a little bit. And that makes perfect sense: we need to get $y$ higher, closer to 1. If we increase $w_1$, that will enforce the input $2$ and if we increase $w_2$, we perform a smaller subtraction - again leaving us with a higher value for $h$ and thus $y$.</p>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg></span>
   <span class="Toast-content">The perspective of neural nets as that of multiple differentiable functions composed together is so prevalent that some people suggested to drop the term 'neural network' or even 'deep learning' in favor of differentiable programming.</span>
</div>

<h1 id="conclusion">Conclusion</h1>

<p>Neural networks perform a calculation of a function composed of many simpler ones to transform an input into an output (e.g., a classification). During training, we need access to partial derivatives to perform parameter updates based on them. We can algorithmically calculate these derivatives and performed some experiments ourselves using a plain Python program. Finally, you have connected all necessary dots to proceed with <em>actual</em> implementations of automatic differentiation. Have fun!</p>

  </div> 
<h1>Comments</h1>
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = /2020/02/16/automatic-differentiation.html;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = /2020/02/16/automatic-differentiation; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://https-alexander-schiendorfer-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                            
<a class="u-url" href="/2020/02/16/automatic-differentiation.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Connecting deep dots</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Connecting deep dots</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list">
  <li><a href="https://github.com/Alexander-Schiendorfer"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">Alexander-Schiendorfer</span></a></li><li><a href="https://www.linkedin.com/in/alexander-schiendorfer"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">alexander-schiendorfer</span></a></li><li><a href="https://www.twitter.com/schienal"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">schienal</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>A blog dedicated to important ideas in deep learning. Focus on intuition and examples.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
